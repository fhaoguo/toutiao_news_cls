{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a728f84",
   "metadata": {},
   "source": [
    "# 头条新闻分类Bert Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb09d79",
   "metadata": {},
   "source": [
    "## BERT要点\n",
    "#### BERT why self-attention\n",
    "+ 计算复杂度，self-attention每层的复杂度O(n^2*d) n是句子的长度，d是词向量维度\n",
    "![](./table1.png)\n",
    "+ 可以并行\n",
    "+ 长程依赖，LSTM任意两点之间需要经过一定的距离，Attention任意两点之间可以直接进行计算。\n",
    "\n",
    "#### 主要贡献\n",
    "+ BERT使用掩码语言模型，可以使得预训练模型进行双向表示\n",
    "+ BERT是第一个基于微调的模型\n",
    "\n",
    "#### Task 1: MASKed LM(遮蔽语言模型)\n",
    "为了训练双向深度表示，我们按照百分比（15%）随机遮盖一些token，然后仅预测这些别遮盖的词。\n",
    "被掩盖的词中，\n",
    "1. 80%的词 被替换成 [MASK]\n",
    "2. 10%的词 被随机替换\n",
    "3. 10%的词 不动\n",
    "\n",
    "#### Task2：Next Sentence Prediction \n",
    "(A, B) 其中B有50%的概率是A的下一句，50%的概率是从数据集中随机选择的一句。\n",
    "如果B是A的下一句标注成isNexT，不是则被标注成NotNext。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea76173",
   "metadata": {},
   "source": [
    "## 编写配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67510c49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2022"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "\n",
    "config = {\n",
    "    'train_file_path': '../../../data/toutiao_news_cls/train.csv',\n",
    "    'test_file_path': '../../../data/toutiao_news_cls/test.csv',\n",
    "    'train_val_ratio': 0.1,  # 10%用作验证集\n",
    "    # ------ 与TextCNN不同的配置 ------\n",
    "    # 'vocab_size': 10000,   # 词典 3W\n",
    "    'head': 'cnn',\n",
    "    'model_path': '../../../pt/bert-base-chinese',\n",
    "    # ------ 与TextCNN不同的配置 ------\n",
    "    'batch_size': 16,      # batch 大小 16\n",
    "    'num_epochs': 1,      # 10次迭代\n",
    "    'learning_rate': 2e-5, # 学习率\n",
    "    'logging_step': 500,   # 每跑300个batch记录一次\n",
    "    'seed': 2022           # 随机种子\n",
    "}\n",
    "\n",
    "config['device'] = 'cuda' if torch.cuda.is_available() else 'cpu' # cpu&gpu\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    return seed\n",
    "\n",
    "seed_everything(config['seed'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e48bf8",
   "metadata": {},
   "source": [
    "## 数据预处理并编写DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c6d34a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bfa4ac85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bert分词器\n",
    "bertTokenizer = BertTokenizer.from_pretrained(config['model_path'])\n",
    "# 重写分词器\n",
    "def tokenizer(sent):\n",
    "    inputs = bertTokenizer.encode_plus(sent, add_special_tokens=True, return_token_type_ids=True, return_attention_mask=True)\n",
    "    \n",
    "    return inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b78eb8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(config, mode='train'):\n",
    "    \n",
    "    data_df = pd.read_csv(config[f'{mode}_file_path'], sep=',')\n",
    "    LABEL, SENTENCE = 'label', 'sentence'\n",
    "    data_df['bert_encode'] = data_df[SENTENCE].apply(tokenizer)\n",
    "    data_df['input_ids'] = data_df['bert_encode'].apply(lambda s: s['input_ids'])\n",
    "    input_ids = np.array([[int(id_) for id_ in v] for v in data_df['input_ids'].values])\n",
    "    data_df['token_type_ids'] = data_df['bert_encode'].apply(lambda s: s['token_type_ids'])\n",
    "    token_type_ids = np.array([[int(id_) for id_ in v] for v in data_df['token_type_ids'].values])\n",
    "    data_df['attention_mask'] = data_df['bert_encode'].apply(lambda s: s['attention_mask'])\n",
    "    attention_mask = np.array([[int(id_) for id_ in v] for v in data_df['attention_mask'].values])\n",
    "\n",
    "    if mode == 'train':\n",
    "        labels = data_df[LABEL].values\n",
    "        \n",
    "        X_train, y_train = defaultdict(list), []\n",
    "        X_val, y_val = defaultdict(list), []\n",
    "        num_val = int(config['train_val_ratio'] * len(data_df))\n",
    "        \n",
    "        # shuffle ids\n",
    "        ids = np.random.choice(range(len(data_df)), size=len(data_df), replace=False)\n",
    "        train_ids = ids[num_val:]\n",
    "        val_ids = ids[:num_val]\n",
    "        \n",
    "        # get input_ids\n",
    "        X_train['input_ids'], y_train = input_ids[train_ids], labels[train_ids]\n",
    "        X_val['input_ids'], y_val = input_ids[val_ids], labels[val_ids]\n",
    "         # get token_type_ids\n",
    "        X_train['token_type_ids'] = token_type_ids[train_ids]\n",
    "        X_val['token_type_ids'] = token_type_ids[val_ids]\n",
    "        # get attention_mask\n",
    "        X_train['attention_mask'] = attention_mask[train_ids]\n",
    "        X_val['attention_mask'] = attention_mask[val_ids]\n",
    "     \n",
    "        # label \n",
    "        label2id = {label: i for i, label in enumerate(np.unique(y_train))}\n",
    "        id2label = {i: label for label, i in label2id.items()}\n",
    "        y_train = torch.tensor([label2id[y] for y in y_train], dtype=torch.long)\n",
    "        y_val = torch.tensor([label2id[y] for y in y_val], dtype=torch.long)\n",
    "\n",
    "        return X_train, y_train, X_val, y_val, label2id, id2label\n",
    "\n",
    "    else:\n",
    "        X_test = defaultdict(list)\n",
    "        X_test['input_ids'] = input_ids\n",
    "        X_test['token_type_ids'] = token_type_ids\n",
    "        X_test['attention_mask'] = attention_mask\n",
    "        y_test = torch.zeros(len(data_df), dtype=torch.long)\n",
    "        \n",
    "        return X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2929eb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fenghaoguo/opt/anaconda3/envs/ng/lib/python3.7/site-packages/ipykernel_launcher.py:7: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  import sys\n",
      "/Users/fenghaoguo/opt/anaconda3/envs/ng/lib/python3.7/site-packages/ipykernel_launcher.py:9: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  if __name__ == '__main__':\n",
      "/Users/fenghaoguo/opt/anaconda3/envs/ng/lib/python3.7/site-packages/ipykernel_launcher.py:11: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_val, y_val, label2id, id2label = read_data(config, mode='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68ee4770",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fenghaoguo/opt/anaconda3/envs/ng/lib/python3.7/site-packages/ipykernel_launcher.py:7: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  import sys\n",
      "/Users/fenghaoguo/opt/anaconda3/envs/ng/lib/python3.7/site-packages/ipykernel_launcher.py:9: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  if __name__ == '__main__':\n",
      "/Users/fenghaoguo/opt/anaconda3/envs/ng/lib/python3.7/site-packages/ipykernel_launcher.py:11: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "X_test, y_test = read_data(config, mode='test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6a4c71",
   "metadata": {},
   "source": [
    "#### Dataset提供数据集的封装，创建/继承Dataset必须实现:\n",
    "+ __len__: 整个数据集的长度\n",
    "+ __getitem__: 支持数据集索引的函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa55be38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "class TNEWSDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.x = X\n",
    "        self.y = y\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids' : self.x['input_ids'][idx],\n",
    "            'label' : self.y[idx],\n",
    "            'token_type_ids': self.x['token_type_ids'][idx],\n",
    "            'attention_mask': self.x['attention_mask'][idx]\n",
    "        }\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.y.size(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17714ed3",
   "metadata": {},
   "source": [
    "#### 使用DataLoader实现数据集的并行加载\n",
    "+ DataLoader提供一个可迭代对象，实现数据并行加载，从TNEWSDataset返回一个example，取多次，最后形成一个长度为batch_size的列表examples\n",
    "+ examples的格式：[dict1, dict2, ...]\n",
    "+ collate_fn()将examples中的数据合并为Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c16a1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(examples):\n",
    "    input_ids_lst = []\n",
    "    labels = []\n",
    "    # ------ 与TextCNN不同的地方 ------\n",
    "    token_type_ids_lst = []\n",
    "    attention_mask_lst = []\n",
    "    # ------ 与TextCNN不同的地方 ------\n",
    "\n",
    "    for example in examples:\n",
    "        input_ids_lst.append(example['input_ids'])\n",
    "        labels.append(example['label'])\n",
    "        # ------ 与TextCNN不同的地方 ------\n",
    "        token_type_ids_lst.append(example['token_type_ids'])\n",
    "        attention_mask_lst.append(example['attention_mask'])\n",
    "        # ------ 与TextCNN不同的地方 ------\n",
    "        \n",
    "    # 计算input_ids_lst中最长的句子长度，对齐\n",
    "    max_length = max(len(input_ids) for input_ids in input_ids_lst)\n",
    "    # 定义一个Tensor\n",
    "    input_ids_tensor = torch.zeros((len(labels), max_length), dtype=torch.long)\n",
    "    # ------ 与TextCNN不同的地方 ------\n",
    "    token_type_ids_tensor = torch.zeros_like(input_ids_tensor)\n",
    "    attention_mask_tensor = torch.zeros_like(input_ids_tensor)\n",
    "    # ------ 与TextCNN不同的地方 ------\n",
    "    \n",
    "    for i, input_ids in enumerate(input_ids_lst):\n",
    "        seq_len = len(input_ids)\n",
    "        input_ids_tensor[i, :seq_len] = torch.tensor(input_ids, dtype=torch.long)\n",
    "        # ------ 与TextCNN不同的地方 ------\n",
    "        token_type_ids_tensor[i, :seq_len] = torch.tensor(token_type_ids_lst[i], dtype=torch.long)\n",
    "        attention_mask_tensor[i, :seq_len] = torch.tensor(attention_mask_lst[i], dtype=torch.long)\n",
    "        # ------ 与TextCNN不同的地方 ------\n",
    "        \n",
    "    return {\n",
    "        'input_ids': input_ids_tensor,\n",
    "        'labels': torch.tensor(labels, dtype=torch.long),\n",
    "        # ------ 与TextCNN不同的地方 ------\n",
    "        'token_type_ids': token_type_ids_tensor,\n",
    "        'attention_mask': attention_mask_tensor\n",
    "        # ------ 与TextCNN不同的地方 ------\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41ac9e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def build_dataloader(config):\n",
    "    X_train, y_train, X_val, y_val, label2id, id2label = read_data(config, mode='train')\n",
    "    X_test, y_test = read_data(config, mode='test')\n",
    "    \n",
    "    train_dataset = TNEWSDataset(X_train, y_train)\n",
    "    val_dataset = TNEWSDataset(X_val, y_val)\n",
    "    test_dataset = TNEWSDataset(X_test, y_test)\n",
    "    \n",
    "    train_dataloader = DataLoader(dataset=train_dataset, batch_size=config['batch_size'], num_workers=0, shuffle=True, collate_fn=collate_fn)\n",
    "    val_dataloader = DataLoader(dataset=val_dataset, batch_size=config['batch_size'], num_workers=0, shuffle=False, collate_fn=collate_fn)\n",
    "    test_dataloader = DataLoader(dataset=test_dataset, batch_size=config['batch_size'], num_workers=0, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "    return train_dataloader, val_dataloader, test_dataloader, id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5def9409",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fenghaoguo/opt/anaconda3/envs/ng/lib/python3.7/site-packages/ipykernel_launcher.py:7: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  import sys\n",
      "/Users/fenghaoguo/opt/anaconda3/envs/ng/lib/python3.7/site-packages/ipykernel_launcher.py:9: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  if __name__ == '__main__':\n",
      "/Users/fenghaoguo/opt/anaconda3/envs/ng/lib/python3.7/site-packages/ipykernel_launcher.py:11: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "train_dataloader, val_dataloader, test_dataloader, id2label = build_dataloader(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8be30d6f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 16 16 16\n",
      "{'input_ids': tensor([[ 101,  776,  691, 2356,  966, 5892, 1355, 1283,  783, 8024, 2832, 6598,\n",
      "         5442,  812,  711,  862, 2898, 5330,  976, 4958,  776,  691, 8043,  102,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [ 101,  677, 5468, 8038, 7599, 3235, 1915, 7455, 3449,  771, 7676, 8024,\n",
      "          678, 5468, 2582,  720, 2190, 8043,  102,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [ 101, 1266,  776, 1957, 2094, 1745, 7063, 8038, 4385, 2141, 5445, 3655,\n",
      "         6999, 8024, 6821, 2218, 3221, 4495, 3833, 8013,  102,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [ 101,  100, 2207, 1285, 1159,  100, 3173, 3124, 5862, 1765, 2157, 7270,\n",
      "         4193, 5991, 6421,  679, 6421, 5314, 2015, 2845,  702, 4408,  102,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [ 101, 2809, 3124,  671, 1453, 2399, 8024, 7716, 1046, 7987, 6158, 6585,\n",
      "          677,  749, 1525,  763, 3403, 5041, 8043,  102,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [ 101, 4374, 5442, 5783, 5438, 2571, 6206, 6617, 4638, 3198,  952, 8024,\n",
      "         6651, 6822, 3127, 3175, 3787, 3717, 3221,  711,  749,  784,  720, 8043,\n",
      "          102,    0,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [ 101, 3175, 5660, 2094, 6432, 6496, 3470, 3975, 5632, 1313, 2428, 8024,\n",
      "          704, 1744, 1367,  807, 3766, 3300, 1920, 6496, 8024,  872, 6371,  711,\n",
      "         6821, 4905, 6432, 3791, 2582,  720, 3416, 8043,  102],\n",
      "        [ 101,  877, 5812, 2898, 5330, 6624,  856, 8013, 2255,  691, 7826, 5543,\n",
      "          121,  118,  121, 7315, 2398,  782, 1469, 7231, 1927, 4633, 7553, 5679,\n",
      "         3322,  102,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [ 101,  675, 3678, 2023, 6239, 4821, 1168, 1957, 2053, 2419, 5296, 1957,\n",
      "         2053, 1920, 7733,  675, 3678, 2023, 3291, 2399, 3309, 2207, 2356, 3696,\n",
      "          102,    0,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [ 101, 6929,  763, 1728,  711, 3683, 4294, 2355, 5445,  671, 1915, 3274,\n",
      "         2168, 4638,  782, 1394, 4415, 1408, 8043,  102,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [ 101, 3342, 7885, 6158, 5470,  711,  100, 6808, 2123, 4689, 7344, 4135,\n",
      "         1121, 4135, 1062, 4660, 2501, 6496, 1920,  886,  100, 6205, 6163, 7484,\n",
      "         2252, 2358, 3698, 4007, 1146,  102,    0,    0,    0],\n",
      "        [ 101, 5384, 2357, 3788, 6158, 4917, 3647,  767,  722, 3862, 8024,  868,\n",
      "          711, 2769, 1744, 5018,  671, 3187,  782, 1277, 8024, 1355, 4495, 4638,\n",
      "         4868, 4908,  752,  816, 3300, 1525,  763,  102,    0],\n",
      "        [ 101, 3726, 7415,  686, 4518, 5865, 1399, 6408,  782, 2110, 5442, 3959,\n",
      "         1298, 2360, 5745, 1920, 2110, 2768, 4989, 5739, 5401, 6408, 3625, 4777,\n",
      "         4955,  704, 2552,  102,    0,    0,    0,    0,    0],\n",
      "        [ 101, 7481, 2190, 2106, 2094, 3181, 1469, 4374, 3457, 3815, 4638, 1103,\n",
      "         1140, 8024, 3360,  714, 6421,  862, 1343,  862,  794, 8024, 5018,  671,\n",
      "         5632, 4507,  782, 1377, 1415,  924, 8043,  102,    0],\n",
      "        [ 101,  517, 2769, 4638, 1730, 7270, 2769, 4638, 1730,  518,  704, 8024,\n",
      "         1525,  702, 2658, 5688, 3297, 6375,  872, 2697, 1220, 8043,  102,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [ 101,  691, 7029, 1764, 1434, 2190,  800, 5011,  678, 4638, 1538, 3813,\n",
      "         7434, 4950, 3221, 2577, 4708,  784,  720, 3416, 4638, 2697, 2658, 1450,\n",
      "         8043,  102,    0,    0,    0,    0,    0,    0,    0]]), 'labels': tensor([ 8,  1,  2,  7, 11, 14,  1,  3,  2, 14,  3,  9,  7,  3,  9,  1]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 0, 0, 0, 0, 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "for batch in train_dataloader:\n",
    "    print(len(batch['input_ids']), len(batch['labels']), len(batch['token_type_ids']), len(batch['attention_mask']))\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a32772a",
   "metadata": {},
   "source": [
    "## 训练验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "10cc4ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT + head part2\n",
    "from transformers import BertPreTrainedModel, BertModel\n",
    "\n",
    "class BertForTNEWS(BertPreTrainedModel):\n",
    "    # classifier -- head\n",
    "    def __init__(self, config, model_path, classifier):\n",
    "        super(BertForTNEWS, self).__init__(config)\n",
    "\n",
    "        self.bert = BertModel.from_pretrained(model_path, config=config)\n",
    "        self.classifier = classifier\n",
    "    \n",
    "    def forward(self, input_ids, token_type_ids,  attention_mask, labels):\n",
    "\n",
    "        outputs = self.bert(input_ids=input_ids,\n",
    "                            attention_mask=attention_mask, \n",
    "                            token_type_ids=token_type_ids, \n",
    "                            output_hidden_states=True)\n",
    "        \n",
    "        hidden_states = outputs[2]\n",
    "\n",
    "        logits = self.classifier(hidden_states, input_ids)\n",
    "        \n",
    "        outputs =(logits, )\n",
    "        # 使用训练集、验证集\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits, labels.view(-1))\n",
    "            outputs =(loss, ) + outputs\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "adc966f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "class ConvClassifier(nn.Module):\n",
    "    '''\n",
    "    CNN + global max pool\n",
    "    '''\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv1d(in_channels=config.hidden_size, out_channels=config.hidden_size, kernel_size=3)\n",
    "        self.global_max_pool = nn.AdaptiveMaxPool1d(1)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.fc = nn.Linear(config.hidden_size, config.num_labels)\n",
    "    \n",
    "    def forward(self, hidden_states, input_ids):\n",
    "        hidden_states = self.dropout(hidden_states[-1])#只取出最后一层\n",
    "        # hidden_states shape (bs, seq_len, hidden_size) -> (bs, hidden_size, seq_len) \n",
    "        hidden_states = hidden_states.permute(0, 2, 1)\n",
    "        out = F.relu(self.conv(hidden_states))\n",
    "        \n",
    "        # out (bs, hidden_size_out, seq_len_out)\n",
    "        # out (bs, hidden_size, 1)\n",
    "        # out (bs, hidden_size)\n",
    "        out = self.global_max_pool(out).squeeze(dim=2)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e1006164",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(model_path, config, head):\n",
    "    heads = {\n",
    "        'cnn':ConvClassifier\n",
    "    }\n",
    "    model = BertForTNEWS(config, model_path, heads[head](config))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "62e2a30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def evaluation(config, model, val_dataloader):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    labels = []\n",
    "    val_loss = 0.\n",
    "    val_iterator = tqdm(val_dataloader, desc='Evaluation...', total=len(val_dataloader))\n",
    "    with torch.no_grad():\n",
    "        for batch in val_iterator:\n",
    "            labels.append(batch['labels'])\n",
    "            batch = {item:value.to(config['device']) for item, value in batch.items()}\n",
    "            \n",
    "            # val output (loss, out)\n",
    "            loss, logits = model(**batch)[:2]\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            preds.append(logits.argmax(dim=-1).detach().cpu())\n",
    "            \n",
    "    avg_val_loss = val_loss/len(val_dataloader)\n",
    "    labels = torch.cat(labels, dim=0).numpy()\n",
    "    preds = torch.cat(preds, dim=0).numpy()\n",
    "    \n",
    "    f1 =f1_score(labels, preds, average='macro')\n",
    "    \n",
    "    return avg_val_loss, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6614940a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bert model train\n",
    "from transformers import BertConfig, BertForSequenceClassification\n",
    "from transformers import AdamW\n",
    "from tqdm import trange\n",
    "\n",
    "def train(config, train_dataloader, val_dataloader, model):\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=config['learning_rate'])\n",
    "    \n",
    "    model.to(config['device'])\n",
    "    \n",
    "    epoches_iterator = trange(config['num_epochs'])\n",
    "    global_steps = 0\n",
    "    train_loss = 0.\n",
    "    logging_loss = 0.\n",
    "    \n",
    "    for epoch in epoches_iterator:\n",
    "        train_iterator = tqdm(train_dataloader, desc='Training', total=len(train_dataloader))\n",
    "        model.train()\n",
    "        for batch in train_iterator:\n",
    "            batch = {item:value.to(config['device']) for item, value in batch.items()}\n",
    "            \n",
    "            # train output (loss, out)\n",
    "            loss = model(**batch)[0]\n",
    "            \n",
    "            model.zero_grad()  # 模型参数梯度清零\n",
    "            loss.backward()  # 反向传播\n",
    "            optimizer.step()  # 更新参数\n",
    "            train_loss += loss.item()  # 叠加loss\n",
    "            global_steps += 1\n",
    "            \n",
    "            if global_steps % config['logging_step'] == 0:\n",
    "                print_train_loss = (train_loss - logging_loss) / config['logging_step']\n",
    "                logging_loss = train_loss\n",
    "                avg_val_loss, f1 = evaluation(config, model, val_dataloader)\n",
    "                print_log = f'>>> training loss: {print_train_loss: .4f}, valid loss: {avg_val_loss: .4f}, valid f1 score: {f1: .4f}'\n",
    "                print(print_log)\n",
    "                model.train()\n",
    "                \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "99bf9e47",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bert_config' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/19/dgxwsbgd76728hx577833nx00000gn/T/ipykernel_1109/2031525661.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbert_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid2label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# 首次运行代码\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mbert_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model_path'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model_path'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbert_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'head'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'bert_config' is not defined"
     ]
    }
   ],
   "source": [
    "bert_config.num_labels = len(id2label)\n",
    "\n",
    "# 首次运行代码\n",
    "bert_config = BertConfig.from_pretrained(config['model_path'])\n",
    "model = build_model(config['model_path'], bert_config, config['head'])\n",
    "best_model = train(config, train_dataloader, val_dataloader, model)\n",
    "best_model.save_pretrained('../../../pt_tmp/bert_head_base_chinese')\n",
    "\n",
    "# 迭代训练代码\n",
    "# bert_config = BertConfig.from_pretrained('../../../pt_tmp/bert_head_base_chinese')\n",
    "# model = BertModel.from_pretrained('../../../pt_tmp/bert_head_base_chinese', config=bert_config)\n",
    "# best_model = train(config, train_dataloader, val_dataloader, model)\n",
    "# best_model.save_pretrained('../../../pt_tmp/bert_base_chinese')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8c2a95",
   "metadata": {},
   "source": [
    "## 预测并保存结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e47176d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(config, id2label, model, test_dataloader):\n",
    "    test_iterator = tqdm(test_dataloader, desc='Testing', total=len(test_dataloader))\n",
    "    model.eval()\n",
    "    test_preds = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in test_iterator:\n",
    "            batch = {item: value.to(config['device']) for item, value in batch.items()}\n",
    "\n",
    "            logits = model(**batch)[1]\n",
    "            test_preds.append(logits.argmax(dim=-1).detach().cpu())\n",
    "            \n",
    "    test_preds = torch.cat(test_preds, dim=0).numpy()\n",
    "    test_preds = [id2label[id_] for id_ in test_preds]\n",
    "        \n",
    "    test_df = pd.read_csv(config['test_file_path'], sep=',')\n",
    "    # test_df.insert(1, column=['label_pred'], value=test_preds)\n",
    "    test_df['label_pred'] = test_preds\n",
    "    # test_df.drop(columns=['sentence'], inplace=True)\n",
    "    test_df.to_csv('submission.csv', index=False, encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "60fde8bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|████████████████████████████████████████████| 625/625 [10:52<00:00,  1.04s/it]\n"
     ]
    }
   ],
   "source": [
    "predict(config, id2label, best_model, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "98bf771a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(config['test_file_path'], sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a4364da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(config['train_file_path'], sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9d665174",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>label_desc</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>108</td>\n",
       "      <td>news_edu</td>\n",
       "      <td>上课时学生手机响个不停，老师一怒之下把手机摔了，家长拿发票让老师赔，大家怎么看待这种事？</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>104</td>\n",
       "      <td>news_finance</td>\n",
       "      <td>商赢环球股份有限公司关于延期回复上海证券交易所对公司2017年年度报告的事后审核问询函的公告</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>106</td>\n",
       "      <td>news_house</td>\n",
       "      <td>通过中介公司买了二手房，首付都付了，现在卖家不想卖了。怎么处理？</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>112</td>\n",
       "      <td>news_travel</td>\n",
       "      <td>2018年去俄罗斯看世界杯得花多少钱？</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>109</td>\n",
       "      <td>news_tech</td>\n",
       "      <td>剃须刀的个性革新，雷明登天猫定制版新品首发</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>103</td>\n",
       "      <td>news_sports</td>\n",
       "      <td>再次证明了“无敌是多么寂寞”——逆天的中国乒乓球队！</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>109</td>\n",
       "      <td>news_tech</td>\n",
       "      <td>三农盾SACC-全球首个推出：互联网+区块链+农产品的电商平台</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>116</td>\n",
       "      <td>news_game</td>\n",
       "      <td>重做or新英雄？其实重做对暴雪来说同样重要</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>103</td>\n",
       "      <td>news_sports</td>\n",
       "      <td>如何在商业活动中不受人欺骗？</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>101</td>\n",
       "      <td>news_culture</td>\n",
       "      <td>87版红楼梦最温柔的四个丫鬟，娶谁都是一生的福气</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label    label_desc                                        sentence\n",
       "0   0    108      news_edu    上课时学生手机响个不停，老师一怒之下把手机摔了，家长拿发票让老师赔，大家怎么看待这种事？\n",
       "1   1    104  news_finance  商赢环球股份有限公司关于延期回复上海证券交易所对公司2017年年度报告的事后审核问询函的公告\n",
       "2   2    106    news_house                通过中介公司买了二手房，首付都付了，现在卖家不想卖了。怎么处理？\n",
       "3   3    112   news_travel                             2018年去俄罗斯看世界杯得花多少钱？\n",
       "4   4    109     news_tech                           剃须刀的个性革新，雷明登天猫定制版新品首发\n",
       "5   5    103   news_sports                      再次证明了“无敌是多么寂寞”——逆天的中国乒乓球队！\n",
       "6   6    109     news_tech                 三农盾SACC-全球首个推出：互联网+区块链+农产品的电商平台\n",
       "7   7    116     news_game                           重做or新英雄？其实重做对暴雪来说同样重要\n",
       "8   8    103   news_sports                                  如何在商业活动中不受人欺骗？\n",
       "9   9    101  news_culture                        87版红楼梦最温柔的四个丫鬟，娶谁都是一生的福气"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2e7757f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([108, 104, 106, 112, 109, 103, 116, 101, 107, 100, 102, 110, 115,\n",
       "       113, 114])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['label'].unique()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
