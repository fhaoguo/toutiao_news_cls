{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a728f84",
   "metadata": {},
   "source": [
    "# 头条新闻分类NeZha With Head And Focal Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea76173",
   "metadata": {},
   "source": [
    "## 编写配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67510c49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2022"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "\n",
    "config = {\n",
    "    'train_file_path': '../../../data/toutiao_news_cls/train.csv',\n",
    "    'test_file_path': '../../../data/toutiao_news_cls/test.csv',\n",
    "    'train_val_ratio': 0.1,  # 10%用作验证集\n",
    "    'head': 'cnn',\n",
    "    'model_path': '../../../pt/NeZha_model',\n",
    "    'batch_size': 16,      # batch 大小 16\n",
    "    'num_epochs': 1,      # 10次迭代\n",
    "    'warmup_ratio': 0.1,   # warm, Focal Loss优化新增参数   \n",
    "    'learning_rate': 2e-5, # 学习率\n",
    "    'logging_step': 300,   # 每跑300个batch记录一次\n",
    "    'seed': 2022           # 随机种子\n",
    "}\n",
    "\n",
    "config['device'] = 'cuda' if torch.cuda.is_available() else 'cpu' # cpu&gpu\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    return seed\n",
    "\n",
    "seed_everything(config['seed'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e48bf8",
   "metadata": {},
   "source": [
    "## 数据预处理并编写DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c6d34a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bfa4ac85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bert分词器\n",
    "bertTokenizer = BertTokenizer.from_pretrained(config['model_path'])\n",
    "# 重写分词器\n",
    "def tokenizer(sent):\n",
    "    inputs = bertTokenizer.encode_plus(sent, add_special_tokens=True, return_token_type_ids=True, return_attention_mask=True)\n",
    "    \n",
    "    return inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b78eb8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(config, mode='train'):\n",
    "    \n",
    "    data_df = pd.read_csv(config[f'{mode}_file_path'], sep=',')\n",
    "    LABEL, SENTENCE = 'label', 'sentence'\n",
    "    data_df['bert_encode'] = data_df[SENTENCE].apply(tokenizer)\n",
    "    data_df['input_ids'] = data_df['bert_encode'].apply(lambda s: s['input_ids'])\n",
    "    input_ids = np.array([[int(id_) for id_ in v] for v in data_df['input_ids'].values])\n",
    "    data_df['token_type_ids'] = data_df['bert_encode'].apply(lambda s: s['token_type_ids'])\n",
    "    token_type_ids = np.array([[int(id_) for id_ in v] for v in data_df['token_type_ids'].values])\n",
    "    data_df['attention_mask'] = data_df['bert_encode'].apply(lambda s: s['attention_mask'])\n",
    "    attention_mask = np.array([[int(id_) for id_ in v] for v in data_df['attention_mask'].values])\n",
    "\n",
    "    if mode == 'train':\n",
    "        labels = data_df[LABEL].values\n",
    "        \n",
    "        X_train, y_train = defaultdict(list), []\n",
    "        X_val, y_val = defaultdict(list), []\n",
    "        num_val = int(config['train_val_ratio'] * len(data_df))\n",
    "        \n",
    "        # shuffle ids\n",
    "        ids = np.random.choice(range(len(data_df)), size=len(data_df), replace=False)\n",
    "        train_ids = ids[num_val:]\n",
    "        val_ids = ids[:num_val]\n",
    "        \n",
    "        # get input_ids\n",
    "        X_train['input_ids'], y_train = input_ids[train_ids], labels[train_ids]\n",
    "        X_val['input_ids'], y_val = input_ids[val_ids], labels[val_ids]\n",
    "         # get token_type_ids\n",
    "        X_train['token_type_ids'] = token_type_ids[train_ids]\n",
    "        X_val['token_type_ids'] = token_type_ids[val_ids]\n",
    "        # get attention_mask\n",
    "        X_train['attention_mask'] = attention_mask[train_ids]\n",
    "        X_val['attention_mask'] = attention_mask[val_ids]\n",
    "     \n",
    "        # label \n",
    "        label2id = {label: i for i, label in enumerate(np.unique(y_train))}\n",
    "        id2label = {i: label for label, i in label2id.items()}\n",
    "        y_train = torch.tensor([label2id[y] for y in y_train], dtype=torch.long)\n",
    "        y_val = torch.tensor([label2id[y] for y in y_val], dtype=torch.long)\n",
    "\n",
    "        return X_train, y_train, X_val, y_val, label2id, id2label\n",
    "\n",
    "    else:\n",
    "        X_test = defaultdict(list)\n",
    "        X_test['input_ids'] = input_ids\n",
    "        X_test['token_type_ids'] = token_type_ids\n",
    "        X_test['attention_mask'] = attention_mask\n",
    "        y_test = torch.zeros(len(data_df), dtype=torch.long)\n",
    "        \n",
    "        return X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2929eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, y_train, X_val, y_val, label2id, id2label = read_data(config, mode='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68ee4770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test, y_test = read_data(config, mode='test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6a4c71",
   "metadata": {},
   "source": [
    "#### Dataset提供数据集的封装，创建/继承Dataset必须实现:\n",
    "+ __len__: 整个数据集的长度\n",
    "+ __getitem__: 支持数据集索引的函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa55be38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "class TNEWSDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.x = X\n",
    "        self.y = y\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids' : self.x['input_ids'][idx],\n",
    "            'label' : self.y[idx],\n",
    "            'token_type_ids': self.x['token_type_ids'][idx],\n",
    "            'attention_mask': self.x['attention_mask'][idx]\n",
    "        }\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.y.size(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17714ed3",
   "metadata": {},
   "source": [
    "#### 使用DataLoader实现数据集的并行加载\n",
    "+ DataLoader提供一个可迭代对象，实现数据并行加载，从TNEWSDataset返回一个example，取多次，最后形成一个长度为batch_size的列表examples\n",
    "+ examples的格式：[dict1, dict2, ...]\n",
    "+ collate_fn()将examples中的数据合并为Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c16a1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(examples):\n",
    "    input_ids_lst = []\n",
    "    labels = []\n",
    "    # ------ 与TextCNN不同的地方 ------\n",
    "    token_type_ids_lst = []\n",
    "    attention_mask_lst = []\n",
    "    # ------ 与TextCNN不同的地方 ------\n",
    "\n",
    "    for example in examples:\n",
    "        input_ids_lst.append(example['input_ids'])\n",
    "        labels.append(example['label'])\n",
    "        # ------ 与TextCNN不同的地方 ------\n",
    "        token_type_ids_lst.append(example['token_type_ids'])\n",
    "        attention_mask_lst.append(example['attention_mask'])\n",
    "        # ------ 与TextCNN不同的地方 ------\n",
    "        \n",
    "    # 计算input_ids_lst中最长的句子长度，对齐\n",
    "    max_length = max(len(input_ids) for input_ids in input_ids_lst)\n",
    "    # 定义一个Tensor\n",
    "    input_ids_tensor = torch.zeros((len(labels), max_length), dtype=torch.long)\n",
    "    # ------ 与TextCNN不同的地方 ------\n",
    "    token_type_ids_tensor = torch.zeros_like(input_ids_tensor)\n",
    "    attention_mask_tensor = torch.zeros_like(input_ids_tensor)\n",
    "    # ------ 与TextCNN不同的地方 ------\n",
    "    \n",
    "    for i, input_ids in enumerate(input_ids_lst):\n",
    "        seq_len = len(input_ids)\n",
    "        input_ids_tensor[i, :seq_len] = torch.tensor(input_ids, dtype=torch.long)\n",
    "        # ------ 与TextCNN不同的地方 ------\n",
    "        token_type_ids_tensor[i, :seq_len] = torch.tensor(token_type_ids_lst[i], dtype=torch.long)\n",
    "        attention_mask_tensor[i, :seq_len] = torch.tensor(attention_mask_lst[i], dtype=torch.long)\n",
    "        # ------ 与TextCNN不同的地方 ------\n",
    "        \n",
    "    return {\n",
    "        'input_ids': input_ids_tensor,\n",
    "        'labels': torch.tensor(labels, dtype=torch.long),\n",
    "        # ------ 与TextCNN不同的地方 ------\n",
    "        'token_type_ids': token_type_ids_tensor,\n",
    "        'attention_mask': attention_mask_tensor\n",
    "        # ------ 与TextCNN不同的地方 ------\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41ac9e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def build_dataloader(config):\n",
    "    X_train, y_train, X_val, y_val, label2id, id2label = read_data(config, mode='train')\n",
    "    X_test, y_test = read_data(config, mode='test')\n",
    "    \n",
    "    train_dataset = TNEWSDataset(X_train, y_train)\n",
    "    val_dataset = TNEWSDataset(X_val, y_val)\n",
    "    test_dataset = TNEWSDataset(X_test, y_test)\n",
    "    \n",
    "    train_dataloader = DataLoader(dataset=train_dataset, batch_size=config['batch_size'], num_workers=0, shuffle=True, collate_fn=collate_fn)\n",
    "    val_dataloader = DataLoader(dataset=val_dataset, batch_size=config['batch_size'], num_workers=0, shuffle=False, collate_fn=collate_fn)\n",
    "    test_dataloader = DataLoader(dataset=test_dataset, batch_size=config['batch_size'], num_workers=0, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "    return train_dataloader, val_dataloader, test_dataloader, id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5def9409",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fenghaoguo/opt/anaconda3/envs/ng/lib/python3.7/site-packages/ipykernel_launcher.py:7: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  import sys\n",
      "/Users/fenghaoguo/opt/anaconda3/envs/ng/lib/python3.7/site-packages/ipykernel_launcher.py:9: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  if __name__ == '__main__':\n",
      "/Users/fenghaoguo/opt/anaconda3/envs/ng/lib/python3.7/site-packages/ipykernel_launcher.py:11: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "train_dataloader, val_dataloader, test_dataloader, id2label = build_dataloader(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8be30d6f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 16 16 16\n",
      "{'input_ids': tensor([[ 101,  517, 1353, 2607, 6121, 1220,  518, 2972, 1139, 3859, 1930, 4276,\n",
      "         1391, 7883, 3952, 2767, 1070, 4374,  751, 7464, 8024,  872, 2582,  720,\n",
      "         4692, 8043,  102,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0],\n",
      "        [ 101, 1920, 6825,  671, 3175, 7415, 1730, 3221, 2582,  720, 1355, 2245,\n",
      "         6629, 3341, 4638, 8043, 3300,  749, 6237, 1355, 2245, 1380, 4638,  720,\n",
      "         8043,  102,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0],\n",
      "        [ 101, 4767, 7340, 1344, 5709, 3492, 3420, 4905, 2458, 6792, 5636, 2168,\n",
      "         6662,  102,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0],\n",
      "        [ 101, 2809,  689, 5790, 2360, 4692, 6814, 3341, 8024,  872,  812, 3297,\n",
      "         1068, 2552, 4638, 1762, 6821,  749,  102,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0],\n",
      "        [ 101,  100,  758, 1724,  100, 3309, 7313, 8024, 7946, 2255, 7349, 1140,\n",
      "         2773, 5279, 2573, 7667, 4638,  778, 4157,  102,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0],\n",
      "        [ 101, 6370,  749, 1139, 1744, 3952, 6762, 4873, 8024, 2844, 4212, 6814,\n",
      "         3309,  749, 8024, 6133, 1215, 6206, 1914,  719, 2897, 1168, 8024, 5543,\n",
      "         1343, 1408, 8043,  102,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0],\n",
      "        [ 101, 4906, 2949, 4289, 3837, 7987, 6822, 1092, 8038, 1920,  816, 4289,\n",
      "         3837, 4638, 1158, 3173, 1355, 2245,  102,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0],\n",
      "        [ 101, 1196, 5299, 3221, 1963,  862, 2875, 5470, 4028, 1447, 4638, 8043,\n",
      "          102,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0],\n",
      "        [ 101, 2554, 1093, 3333, 8188, 1400, 4638, 1159,  704, 4495, 3889,  102,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0],\n",
      "        [ 101, 1290,  711, 6783,  749, 8711, 3403, 1114,  833, 2193, 5636, 2769,\n",
      "          812,  704, 1744,  821,  689, 1927, 1343,  784,  720, 8043,  102,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0],\n",
      "        [ 101, 3189, 3315, 3300, 3766, 3300, 5543, 1213,  794,  915, 5384, 3172,\n",
      "         2797,  704, 2843, 1726, 1266, 3175, 1724, 2270, 8043,  102,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0],\n",
      "        [ 101, 4343,  817, 6649, 6649,  679,  828,  862, 3198, 5543, 1168, 2419,\n",
      "         8043,  143, 5500, 3297, 5283, 2248, 2153, 5500, 1290, 5320, 5500,  819,\n",
      "         5357,  862, 6834, 1158, 3173, 7770, 8043,  102,    0,    0,    0,    0,\n",
      "            0],\n",
      "        [ 101, 2408, 2336, 2608, 1920, 4638, 6948, 3255, 3297, 6818,  711,  784,\n",
      "          720,  671, 4684, 3766, 3300, 6822, 1057, 3683, 6612, 1399, 1296, 8024,\n",
      "         3221, 1358,  839, 6820, 3221,  784,  720, 1166, 4638, 1333, 1728, 8043,\n",
      "          102],\n",
      "        [ 101,  677, 5468, 8038,  671, 2428, 3217, 7599,  671, 2428, 2687, 8024,\n",
      "         7942, 5709, 5862, 2226, 8024, 1963,  862, 2190,  678, 5468, 8043,  102,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0],\n",
      "        [ 101, 2349, 5838, 4294, 8038, 3187,  782, 7730, 7724, 7585, 6208,  772,\n",
      "          689, 3419, 2229,  924, 7372, 1062, 1385, 2199, 2689, 1355, 1737, 7410,\n",
      "          102,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0],\n",
      "        [ 101, 1920, 2349, 7944,  697, 4413, 3215, 6158, 1398,  671, 2157,  936,\n",
      "          727, 6956, 4681,  677, 8013,  122,  783, 3616, 3766, 1168, 2797, 2218,\n",
      "         1762, 2682, 2582,  720, 5709, 8013,  102,    0,    0,    0,    0,    0,\n",
      "            0]]), 'labels': tensor([14,  5, 13,  4,  1, 10,  8,  2,  7,  8,  9, 13,  3,  1,  8,  3]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "for batch in train_dataloader:\n",
    "    print(len(batch['input_ids']), len(batch['labels']), len(batch['token_type_ids']), len(batch['attention_mask']))\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a32772a",
   "metadata": {},
   "source": [
    "## 训练验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "10cc4ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NeZha + head part2\n",
    "from NeZha import *\n",
    "from extra_loss import *\n",
    "\n",
    "class NeZhaForTNEWS(NeZhaPreTrainedModel):\n",
    "    # classifier -- head\n",
    "    def __init__(self, config, model_path, classifier):\n",
    "        super(NeZhaForTNEWS, self).__init__(config)\n",
    "\n",
    "        self.bert = NeZhaModel.from_pretrained(model_path, config=config)\n",
    "        self.classifier = classifier  # head\n",
    "        self.config = config  # Focal Loss 优化新增代码\n",
    "    \n",
    "    def forward(self,\n",
    "                input_ids: torch.Tensor=None,\n",
    "                token_type_ids: torch.Tensor=None,\n",
    "                attention_mask: torch.Tensor=None,\n",
    "                labels: torch.Tensor=None):\n",
    "\n",
    "        outputs = self.bert(input_ids=input_ids,\n",
    "                            attention_mask=attention_mask, \n",
    "                            token_type_ids=token_type_ids)\n",
    "        \n",
    "        hidden_states = outputs[2]\n",
    "        \n",
    "        logits = self.classifier(hidden_states, input_ids)\n",
    "        \n",
    "        outputs =(logits, )\n",
    "        # 使用训练集、验证集\n",
    "        if labels is not None:\n",
    "            # Focal Loss 损失计算优化代码\n",
    "            # loss_fct = nn.CrossEntropyLoss()\n",
    "            loss_fct = FocalLoss(num_classes=self.config.num_labels)\n",
    "            loss = loss_fct(logits, labels.view(-1))\n",
    "            outputs =(loss, ) + outputs\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "adc966f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "class ConvClassifier(nn.Module):\n",
    "    '''\n",
    "    CNN + global max pool\n",
    "    '''\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv1d(in_channels=config.hidden_size, out_channels=config.hidden_size, kernel_size=3)\n",
    "        self.global_max_pool = nn.AdaptiveMaxPool1d(1)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.fc = nn.Linear(config.hidden_size, config.num_labels)\n",
    "    \n",
    "    def forward(self, hidden_states, input_ids):\n",
    "        hidden_states = self.dropout(hidden_states[-1])#只取出最后一层\n",
    "        # hidden_states shape (bs, seq_len, hidden_size) -> (bs, hidden_size, seq_len) \n",
    "        hidden_states = hidden_states.permute(0, 2, 1)\n",
    "        out = F.relu(self.conv(hidden_states))\n",
    "        \n",
    "        # out (bs, hidden_size_out, seq_len_out)\n",
    "        # out (bs, hidden_size, 1)\n",
    "        # out (bs, hidden_size)\n",
    "        out = self.global_max_pool(out).squeeze(dim=2)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e1006164",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(model_path, config, head):\n",
    "    heads = {\n",
    "        'cnn':ConvClassifier\n",
    "    }\n",
    "    assert head in heads, \"@_@:head must have been implemented!\"\n",
    "    print(f'>>>You are using {head} head ...')\n",
    "    model = NeZhaForTNEWS(config, model_path, heads[head](config))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "62e2a30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "\n",
    "def evaluation(config, model, val_dataloader):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    labels = []\n",
    "    val_loss = 0.\n",
    "    val_iterator = tqdm(val_dataloader, desc='Evaluation...', total=len(val_dataloader))\n",
    "    with torch.no_grad():\n",
    "        for batch in val_iterator:\n",
    "            labels.append(batch['labels'])\n",
    "            batch = {item:value.to(config['device']) for item, value in batch.items()}\n",
    "            \n",
    "            # val output (loss, out)\n",
    "            loss, logits = model(**batch)[:2]\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            preds.append(logits.argmax(dim=-1).detach().cpu())\n",
    "            \n",
    "    avg_val_loss = val_loss/len(val_dataloader)\n",
    "    labels = torch.cat(labels, dim=0).numpy()\n",
    "    preds = torch.cat(preds, dim=0).numpy()\n",
    "    \n",
    "    precision = precision_score(labels, preds, average='macro')\n",
    "    recall = recall_score(labels, preds, average='macro')\n",
    "    f1 =f1_score(labels, preds, average='macro')\n",
    "    accuracy = accuracy_score(labels, preds)\n",
    "    \n",
    "    return avg_val_loss, f1, precision, recall, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6614940a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NeZha model + Head train\n",
    "from transformers import BertConfig, BertForSequenceClassification\n",
    "from transformers import AdamW\n",
    "from tqdm import trange\n",
    "\n",
    "from extra_optim import *\n",
    "\n",
    "def train(config, train_dataloader, val_dataloader, model):\n",
    "\n",
    "    # Focal Loss 使用的优化器\n",
    "    # 得到模型的参数\n",
    "    optimizer_grouped_parameters = model.parameters()\n",
    "    # 定义一个基优化器\n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr=config['learning_rate'])\n",
    "    # Lookahead要有一个基优化器，k=5，alpha=1\n",
    "    optimizer = Lookahead(optimizer, 5, 1)\n",
    "    total_steps = config['num_epochs'] * len(train_dataloader)\n",
    "    # 每调用warmup_steps次，对应的学习率就会调整一次\n",
    "    lr_scheduler = WarmupLinearSchedule(optimizer, \n",
    "                                        warmup_steps=int(config['warmup_ratio'] * total_steps),\n",
    "                                        t_total=total_steps)\n",
    "    # Focal Loss End\n",
    "    \n",
    "    model.to(config['device'])\n",
    "    \n",
    "    epoches_iterator = trange(config['num_epochs'])\n",
    "    global_steps = 0\n",
    "    train_loss = 0.\n",
    "    logging_loss = 0.\n",
    "    \n",
    "    best_f1 = 0.\n",
    "    best_precision = 0.\n",
    "    best_recall = 0.\n",
    "    best_accuracy = 0.\n",
    "    \n",
    "    for epoch in epoches_iterator:\n",
    "        train_iterator = tqdm(train_dataloader, desc='Training', total=len(train_dataloader))\n",
    "        model.train()\n",
    "        for batch in train_iterator:\n",
    "            batch = {item:value.to(config['device']) for item, value in batch.items()}\n",
    "            \n",
    "            # train output (loss, out)\n",
    "            loss = model(**batch)[0]\n",
    "            \n",
    "            model.zero_grad()  # 模型参数梯度清零\n",
    "            loss.backward()  # 反向传播\n",
    "            \n",
    "            optimizer.step()  # 更新参数\n",
    "            lr_scheduler.step() # Focal Loss优化增加代码\n",
    "            \n",
    "            train_loss += loss.item()  # 叠加loss\n",
    "            global_steps += 1\n",
    "            \n",
    "            if global_steps % config['logging_step'] == 0:\n",
    "                print_train_loss = (train_loss - logging_loss) / config['logging_step']\n",
    "                logging_loss = train_loss\n",
    "                avg_val_loss, f1, precision, recall, accuracy = evaluation(config, model, val_dataloader)\n",
    "                \n",
    "                if best_f1 < f1:\n",
    "                    best_f1 = f1\n",
    "                    best_precision = precision\n",
    "                    best_recall = recall\n",
    "                    best_accuracy = accuracy\n",
    "                    print_log = f'''>>> training loss: {print_train_loss: .4f}, valid loss: {avg_val_loss: .4f}\\n\n",
    "                            valid f1 score: {f1: .4f}, valid precision score: {precision: .4f},\n",
    "                            valid recall score: {recall: .4f}, valid accuracy score: {accuracy: .4f}'''\n",
    "                    print(print_log)\n",
    "                    model.save_pretrained('../../../pt_tmp/cls/nezha_head_focal_loss')\n",
    "                    \n",
    "                model.train()\n",
    "                \n",
    "    return best_f1, best_precision, best_recall, best_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "99bf9e47",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>You are using cnn head ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../../../pt/NeZha_model were not used when initializing NeZhaModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing NeZhaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing NeZhaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of NeZhaModel were not initialized from the model checkpoint at ../../../pt/NeZha_model and are newly initialized: ['bert.encoder.layer.1.attention.self.relative_positions_encoding.positions_encoding', 'bert.encoder.layer.4.attention.self.relative_positions_encoding.positions_encoding', 'bert.encoder.layer.9.attention.self.relative_positions_encoding.positions_encoding', 'bert.encoder.layer.6.attention.self.relative_positions_encoding.positions_encoding', 'bert.encoder.layer.5.attention.self.relative_positions_encoding.positions_encoding', 'bert.encoder.layer.10.attention.self.relative_positions_encoding.positions_encoding', 'bert.encoder.layer.7.attention.self.relative_positions_encoding.positions_encoding', 'bert.encoder.layer.3.attention.self.relative_positions_encoding.positions_encoding', 'bert.encoder.layer.0.attention.self.relative_positions_encoding.positions_encoding', 'bert.encoder.layer.2.attention.self.relative_positions_encoding.positions_encoding', 'bert.encoder.layer.8.attention.self.relative_positions_encoding.positions_encoding', 'bert.encoder.layer.11.attention.self.relative_positions_encoding.positions_encoding']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/fenghaoguo/opt/anaconda3/envs/ng/lib/python3.7/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "  0%|                                                                                                                                                              | 0/1 [00:00<?, ?it/s]\n",
      "Training:   0%|                                                                                                                                                 | 0/3002 [00:00<?, ?it/s]\u001b[A/Users/fenghaoguo/opt/anaconda3/envs/ng/lib/python3.7/site-packages/transformers/modeling_utils.py:700: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  \"The `device` argument is deprecated and will be removed in v5 of Transformers.\", FutureWarning\n",
      "\n",
      "Training:   0%|                                                                                                                                       | 1/3002 [00:04<3:44:18,  4.48s/it]\u001b[A\n",
      "Training:   0%|                                                                                                                                       | 2/3002 [00:08<3:30:33,  4.21s/it]\u001b[A\n",
      "Training:   0%|▏                                                                                                                                      | 3/3002 [00:12<3:16:03,  3.92s/it]\u001b[A\n",
      "Training:   0%|▏                                                                                                                                      | 4/3002 [00:16<3:23:41,  4.08s/it]\u001b[A\n",
      "Training:   0%|▏                                                                                                                                      | 5/3002 [00:19<3:10:57,  3.82s/it]\u001b[A\n",
      "Training:   0%|▎                                                                                                                                      | 6/3002 [00:23<3:11:41,  3.84s/it]\u001b[A\n",
      "Training:   0%|▎                                                                                                                                      | 7/3002 [00:27<3:07:45,  3.76s/it]\u001b[A\n",
      "Training:   0%|▎                                                                                                                                      | 8/3002 [00:31<3:13:04,  3.87s/it]\u001b[A\n",
      "Training:   0%|▍                                                                                                                                      | 9/3002 [00:35<3:13:18,  3.88s/it]\u001b[A\n",
      "Training:   0%|▍                                                                                                                                     | 10/3002 [00:38<3:09:52,  3.81s/it]\u001b[A\n",
      "Training:   0%|▍                                                                                                                                     | 11/3002 [00:42<3:13:23,  3.88s/it]\u001b[A\n",
      "Training:   0%|▌                                                                                                                                     | 12/3002 [00:46<3:09:26,  3.80s/it]\u001b[A\n",
      "Training:   0%|▌                                                                                                                                     | 13/3002 [00:50<3:04:09,  3.70s/it]\u001b[A\n",
      "Training:   0%|▌                                                                                                                                     | 14/3002 [00:53<3:02:21,  3.66s/it]\u001b[A\n",
      "Training:   0%|▋                                                                                                                                     | 15/3002 [00:57<3:09:09,  3.80s/it]\u001b[A\n",
      "Training:   1%|▋                                                                                                                                     | 16/3002 [01:01<3:13:09,  3.88s/it]\u001b[A\n",
      "Training:   1%|▊                                                                                                                                     | 17/3002 [01:05<3:16:19,  3.95s/it]\u001b[A\n",
      "Training:   1%|▊                                                                                                                                     | 18/3002 [01:09<3:08:45,  3.80s/it]\u001b[A\n",
      "Training:   1%|▊                                                                                                                                     | 19/3002 [01:13<3:14:17,  3.91s/it]\u001b[A\n",
      "Training:   1%|▉                                                                                                                                     | 20/3002 [01:18<3:29:48,  4.22s/it]\u001b[A\n",
      "Training:   1%|▉                                                                                                                                     | 21/3002 [01:22<3:33:21,  4.29s/it]\u001b[A\n",
      "Training:   1%|▉                                                                                                                                     | 22/3002 [01:26<3:23:07,  4.09s/it]\u001b[A\n",
      "Training:   1%|█                                                                                                                                     | 23/3002 [01:30<3:20:47,  4.04s/it]\u001b[A\n",
      "Training:   1%|█                                                                                                                                     | 24/3002 [01:35<3:28:24,  4.20s/it]\u001b[A\n",
      "Training:   1%|█                                                                                                                                     | 25/3002 [01:38<3:21:10,  4.05s/it]\u001b[A\n",
      "Training:   1%|█▏                                                                                                                                    | 26/3002 [01:43<3:28:15,  4.20s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|█▏                                                                                                                                    | 27/3002 [01:49<3:51:20,  4.67s/it]\u001b[A\n",
      "Training:   1%|█▏                                                                                                                                    | 28/3002 [01:52<3:38:10,  4.40s/it]\u001b[A\n",
      "Training:   1%|█▎                                                                                                                                    | 29/3002 [01:56<3:31:46,  4.27s/it]\u001b[A\n",
      "Training:   1%|█▎                                                                                                                                    | 30/3002 [02:00<3:29:16,  4.22s/it]\u001b[A\n",
      "Training:   1%|█▍                                                                                                                                    | 31/3002 [02:04<3:26:59,  4.18s/it]\u001b[A\n",
      "Training:   1%|█▍                                                                                                                                    | 32/3002 [02:08<3:20:09,  4.04s/it]\u001b[A\n",
      "Training:   1%|█▍                                                                                                                                    | 33/3002 [02:12<3:14:29,  3.93s/it]\u001b[A\n",
      "Training:   1%|█▌                                                                                                                                    | 34/3002 [02:16<3:23:05,  4.11s/it]\u001b[A\n",
      "Training:   1%|█▌                                                                                                                                    | 35/3002 [02:21<3:27:24,  4.19s/it]\u001b[A\n",
      "Training:   1%|█▌                                                                                                                                    | 36/3002 [02:25<3:25:23,  4.15s/it]\u001b[A\n",
      "Training:   1%|█▋                                                                                                                                    | 37/3002 [02:29<3:24:57,  4.15s/it]\u001b[A\n",
      "Training:   1%|█▋                                                                                                                                    | 38/3002 [02:33<3:26:33,  4.18s/it]\u001b[A\n",
      "Training:   1%|█▋                                                                                                                                    | 39/3002 [02:37<3:21:15,  4.08s/it]\u001b[A\n",
      "Training:   1%|█▊                                                                                                                                    | 40/3002 [02:41<3:16:33,  3.98s/it]\u001b[A\n",
      "Training:   1%|█▊                                                                                                                                    | 41/3002 [02:45<3:21:22,  4.08s/it]\u001b[A\n",
      "Training:   1%|█▊                                                                                                                                    | 42/3002 [02:49<3:14:40,  3.95s/it]\u001b[A\n",
      "Training:   1%|█▉                                                                                                                                    | 43/3002 [02:52<3:10:20,  3.86s/it]\u001b[A\n",
      "Training:   1%|█▉                                                                                                                                    | 44/3002 [02:56<3:10:59,  3.87s/it]\u001b[A\n",
      "Training:   1%|██                                                                                                                                    | 45/3002 [03:01<3:15:36,  3.97s/it]\u001b[A\n",
      "Training:   2%|██                                                                                                                                    | 46/3002 [03:05<3:17:46,  4.01s/it]\u001b[A\n",
      "Training:   2%|██                                                                                                                                    | 47/3002 [03:08<3:13:09,  3.92s/it]\u001b[A\n",
      "Training:   2%|██▏                                                                                                                                   | 48/3002 [03:12<3:13:29,  3.93s/it]\u001b[A\n",
      "Training:   2%|██▏                                                                                                                                   | 49/3002 [03:16<3:08:44,  3.83s/it]\u001b[A\n",
      "Training:   2%|██▏                                                                                                                                   | 50/3002 [03:20<3:07:11,  3.80s/it]\u001b[A\n",
      "Training:   2%|██▎                                                                                                                                   | 51/3002 [03:24<3:14:48,  3.96s/it]\u001b[A\n",
      "Training:   2%|██▎                                                                                                                                   | 52/3002 [03:27<3:08:16,  3.83s/it]\u001b[A\n",
      "Training:   2%|██▎                                                                                                                                   | 53/3002 [03:32<3:18:07,  4.03s/it]\u001b[A\n",
      "Training:   2%|██▍                                                                                                                                   | 54/3002 [03:36<3:24:02,  4.15s/it]\u001b[A\n",
      "Training:   2%|██▍                                                                                                                                   | 55/3002 [03:40<3:22:15,  4.12s/it]\u001b[A\n",
      "Training:   2%|██▍                                                                                                                                   | 56/3002 [03:45<3:22:05,  4.12s/it]\u001b[A\n",
      "Training:   2%|██▌                                                                                                                                   | 57/3002 [03:48<3:17:09,  4.02s/it]\u001b[A\n",
      "Training:   2%|██▌                                                                                                                                   | 58/3002 [03:53<3:28:33,  4.25s/it]\u001b[A\n",
      "Training:   2%|██▋                                                                                                                                   | 59/3002 [03:57<3:27:25,  4.23s/it]\u001b[A\n",
      "Training:   2%|██▋                                                                                                                                   | 60/3002 [04:01<3:21:28,  4.11s/it]\u001b[A\n",
      "Training:   2%|██▋                                                                                                                                   | 61/3002 [04:05<3:18:21,  4.05s/it]\u001b[A\n",
      "Training:   2%|██▊                                                                                                                                   | 62/3002 [04:10<3:24:24,  4.17s/it]\u001b[A\n",
      "Training:   2%|██▊                                                                                                                                   | 63/3002 [04:13<3:15:27,  3.99s/it]\u001b[A\n",
      "Training:   2%|██▊                                                                                                                                   | 64/3002 [04:18<3:30:34,  4.30s/it]\u001b[A\n",
      "Training:   2%|██▉                                                                                                                                   | 65/3002 [04:22<3:23:45,  4.16s/it]\u001b[A\n",
      "Training:   2%|██▉                                                                                                                                   | 66/3002 [04:26<3:19:48,  4.08s/it]\u001b[A\n",
      "Training:   2%|██▉                                                                                                                                   | 67/3002 [04:30<3:19:45,  4.08s/it]\u001b[A\n",
      "Training:   2%|███                                                                                                                                   | 68/3002 [04:34<3:14:03,  3.97s/it]\u001b[A\n",
      "Training:   2%|███                                                                                                                                   | 69/3002 [04:37<3:11:46,  3.92s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   2%|███                                                                                                                                   | 70/3002 [04:41<3:07:12,  3.83s/it]\u001b[A\n",
      "Training:   2%|███▏                                                                                                                                  | 71/3002 [04:46<3:24:41,  4.19s/it]\u001b[A\n",
      "Training:   2%|███▏                                                                                                                                  | 72/3002 [04:50<3:18:53,  4.07s/it]\u001b[A\n",
      "Training:   2%|███▎                                                                                                                                  | 73/3002 [04:54<3:13:01,  3.95s/it]\u001b[A\n",
      "Training:   2%|███▎                                                                                                                                  | 74/3002 [04:59<3:27:43,  4.26s/it]\u001b[A\n",
      "Training:   2%|███▎                                                                                                                                  | 75/3002 [05:04<3:46:53,  4.65s/it]\u001b[A\n",
      "Training:   3%|███▍                                                                                                                                  | 76/3002 [05:09<3:46:01,  4.63s/it]\u001b[A\n",
      "Training:   3%|███▍                                                                                                                                  | 77/3002 [05:13<3:44:10,  4.60s/it]\u001b[A\n",
      "Training:   3%|███▍                                                                                                                                  | 78/3002 [05:17<3:36:24,  4.44s/it]\u001b[A\n",
      "Training:   3%|███▌                                                                                                                                  | 79/3002 [05:21<3:26:42,  4.24s/it]\u001b[A\n",
      "Training:   3%|███▌                                                                                                                                  | 80/3002 [05:25<3:28:12,  4.28s/it]\u001b[A\n",
      "Training:   3%|███▌                                                                                                                                  | 81/3002 [05:30<3:30:15,  4.32s/it]\u001b[A\n",
      "Training:   3%|███▋                                                                                                                                  | 82/3002 [05:33<3:16:37,  4.04s/it]\u001b[A\n",
      "Training:   3%|███▋                                                                                                                                  | 83/3002 [05:37<3:11:32,  3.94s/it]\u001b[A\n",
      "Training:   3%|███▋                                                                                                                                  | 84/3002 [05:41<3:06:37,  3.84s/it]\u001b[A\n",
      "Training:   3%|███▊                                                                                                                                  | 85/3002 [05:45<3:13:06,  3.97s/it]\u001b[A\n",
      "Training:   3%|███▊                                                                                                                                  | 86/3002 [05:50<3:25:57,  4.24s/it]\u001b[A\n",
      "Training:   3%|███▉                                                                                                                                  | 87/3002 [05:55<3:42:11,  4.57s/it]\u001b[A\n",
      "Training:   3%|███▉                                                                                                                                  | 88/3002 [05:58<3:24:54,  4.22s/it]\u001b[A\n",
      "Training:   3%|███▉                                                                                                                                  | 89/3002 [06:03<3:23:00,  4.18s/it]\u001b[A\n",
      "Training:   3%|████                                                                                                                                  | 90/3002 [06:06<3:16:05,  4.04s/it]\u001b[A\n",
      "Training:   3%|████                                                                                                                                  | 91/3002 [06:10<3:14:57,  4.02s/it]\u001b[A\n",
      "Training:   3%|████                                                                                                                                  | 92/3002 [06:15<3:22:16,  4.17s/it]\u001b[A\n",
      "Training:   3%|████▏                                                                                                                                 | 93/3002 [06:19<3:17:08,  4.07s/it]\u001b[A\n",
      "Training:   3%|████▏                                                                                                                                 | 94/3002 [06:23<3:20:59,  4.15s/it]\u001b[A\n",
      "Training:   3%|████▏                                                                                                                                 | 95/3002 [06:27<3:16:19,  4.05s/it]\u001b[A\n",
      "Training:   3%|████▎                                                                                                                                 | 96/3002 [06:31<3:26:40,  4.27s/it]\u001b[A\n",
      "Training:   3%|████▎                                                                                                                                 | 97/3002 [06:36<3:29:20,  4.32s/it]\u001b[A\n",
      "Training:   3%|████▎                                                                                                                                 | 98/3002 [06:40<3:21:37,  4.17s/it]\u001b[A\n",
      "Training:   3%|████▍                                                                                                                                 | 99/3002 [06:43<3:14:27,  4.02s/it]\u001b[A\n",
      "Training:   3%|████▍                                                                                                                                | 100/3002 [06:48<3:16:21,  4.06s/it]\u001b[A\n",
      "Training:   3%|████▍                                                                                                                                | 101/3002 [06:53<3:38:03,  4.51s/it]\u001b[A\n",
      "Training:   3%|████▌                                                                                                                                | 102/3002 [06:58<3:46:37,  4.69s/it]\u001b[A\n",
      "Training:   3%|████▌                                                                                                                                | 103/3002 [07:02<3:31:32,  4.38s/it]\u001b[A\n",
      "Training:   3%|████▌                                                                                                                                | 104/3002 [07:06<3:22:25,  4.19s/it]\u001b[A\n",
      "Training:   3%|████▋                                                                                                                                | 105/3002 [07:09<3:14:52,  4.04s/it]\u001b[A\n",
      "Training:   4%|████▋                                                                                                                                | 106/3002 [07:13<3:14:21,  4.03s/it]\u001b[A\n",
      "Training:   4%|████▋                                                                                                                                | 107/3002 [07:17<3:12:54,  4.00s/it]\u001b[A\n",
      "Training:   4%|████▊                                                                                                                                | 108/3002 [07:21<3:15:41,  4.06s/it]\u001b[A\n",
      "Training:   4%|████▊                                                                                                                                | 109/3002 [07:25<3:10:03,  3.94s/it]\u001b[A\n",
      "Training:   4%|████▊                                                                                                                                | 110/3002 [07:29<3:04:16,  3.82s/it]\u001b[A\n",
      "Training:   4%|████▉                                                                                                                                | 111/3002 [07:33<3:07:55,  3.90s/it]\u001b[A\n",
      "Training:   4%|████▉                                                                                                                                | 112/3002 [07:37<3:07:13,  3.89s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   4%|█████                                                                                                                                | 113/3002 [07:40<3:06:15,  3.87s/it]\u001b[A\n",
      "Training:   4%|█████                                                                                                                                | 114/3002 [07:44<3:06:42,  3.88s/it]\u001b[A\n",
      "Training:   4%|█████                                                                                                                                | 115/3002 [07:49<3:15:20,  4.06s/it]\u001b[A\n",
      "Training:   4%|█████▏                                                                                                                               | 116/3002 [07:53<3:22:40,  4.21s/it]\u001b[A\n",
      "Training:   4%|█████▏                                                                                                                               | 117/3002 [07:57<3:14:49,  4.05s/it]\u001b[A\n",
      "Training:   4%|█████▏                                                                                                                               | 118/3002 [08:01<3:15:14,  4.06s/it]\u001b[A\n",
      "Training:   4%|█████▎                                                                                                                               | 119/3002 [08:05<3:14:23,  4.05s/it]\u001b[A\n",
      "Training:   4%|█████▎                                                                                                                               | 120/3002 [08:09<3:12:03,  4.00s/it]\u001b[A\n",
      "Training:   4%|█████▎                                                                                                                               | 121/3002 [08:13<3:16:56,  4.10s/it]\u001b[A\n",
      "Training:   4%|█████▍                                                                                                                               | 122/3002 [08:17<3:10:16,  3.96s/it]\u001b[A\n",
      "Training:   4%|█████▍                                                                                                                               | 123/3002 [08:21<3:13:10,  4.03s/it]\u001b[A\n",
      "Training:   4%|█████▍                                                                                                                               | 124/3002 [08:25<3:15:18,  4.07s/it]\u001b[A\n",
      "Training:   4%|█████▌                                                                                                                               | 125/3002 [08:29<3:12:18,  4.01s/it]\u001b[A\n",
      "Training:   4%|█████▌                                                                                                                               | 126/3002 [08:34<3:25:32,  4.29s/it]\u001b[A\n",
      "Training:   4%|█████▋                                                                                                                               | 127/3002 [08:39<3:30:34,  4.39s/it]\u001b[A\n",
      "Training:   4%|█████▋                                                                                                                               | 128/3002 [08:43<3:22:01,  4.22s/it]\u001b[A\n",
      "Training:   4%|█████▋                                                                                                                               | 129/3002 [08:47<3:17:07,  4.12s/it]\u001b[A\n",
      "Training:   4%|█████▊                                                                                                                               | 130/3002 [08:52<3:29:26,  4.38s/it]\u001b[A\n",
      "Training:   4%|█████▊                                                                                                                               | 131/3002 [08:56<3:24:58,  4.28s/it]\u001b[A\n",
      "Training:   4%|█████▊                                                                                                                               | 132/3002 [08:59<3:18:45,  4.16s/it]\u001b[A\n",
      "Training:   4%|█████▉                                                                                                                               | 133/3002 [09:03<3:14:37,  4.07s/it]\u001b[A\n",
      "Training:   4%|█████▉                                                                                                                               | 134/3002 [09:07<3:07:36,  3.93s/it]\u001b[A\n",
      "Training:   4%|█████▉                                                                                                                               | 135/3002 [09:10<2:59:34,  3.76s/it]\u001b[A\n",
      "Training:   5%|██████                                                                                                                               | 136/3002 [09:15<3:06:51,  3.91s/it]\u001b[A\n",
      "Training:   5%|██████                                                                                                                               | 137/3002 [09:18<3:07:38,  3.93s/it]\u001b[A\n",
      "Training:   5%|██████                                                                                                                               | 138/3002 [09:23<3:09:04,  3.96s/it]\u001b[A\n",
      "Training:   5%|██████▏                                                                                                                              | 139/3002 [09:26<3:07:13,  3.92s/it]\u001b[A\n",
      "Training:   5%|██████▏                                                                                                                              | 140/3002 [09:31<3:19:58,  4.19s/it]\u001b[A\n",
      "Training:   5%|██████▏                                                                                                                              | 141/3002 [09:36<3:24:13,  4.28s/it]\u001b[A\n",
      "Training:   5%|██████▎                                                                                                                              | 142/3002 [09:40<3:29:39,  4.40s/it]\u001b[A\n",
      "Training:   5%|██████▎                                                                                                                              | 143/3002 [09:45<3:38:12,  4.58s/it]\u001b[A\n",
      "Training:   5%|██████▍                                                                                                                              | 144/3002 [09:49<3:30:41,  4.42s/it]\u001b[A\n",
      "Training:   5%|██████▍                                                                                                                              | 145/3002 [09:53<3:19:20,  4.19s/it]\u001b[A\n",
      "Training:   5%|██████▍                                                                                                                              | 146/3002 [09:57<3:16:58,  4.14s/it]\u001b[A\n",
      "Training:   5%|██████▌                                                                                                                              | 147/3002 [10:01<3:13:20,  4.06s/it]\u001b[A\n",
      "Training:   5%|██████▌                                                                                                                              | 148/3002 [10:05<3:08:10,  3.96s/it]\u001b[A\n",
      "Training:   5%|██████▌                                                                                                                              | 149/3002 [10:09<3:08:19,  3.96s/it]\u001b[A\n",
      "Training:   5%|██████▋                                                                                                                              | 150/3002 [10:13<3:20:38,  4.22s/it]\u001b[A\n",
      "Training:   5%|██████▋                                                                                                                              | 151/3002 [10:19<3:34:16,  4.51s/it]\u001b[A\n",
      "Training:   5%|██████▋                                                                                                                              | 152/3002 [10:22<3:15:09,  4.11s/it]\u001b[A\n",
      "Training:   5%|██████▊                                                                                                                              | 153/3002 [10:25<3:08:30,  3.97s/it]\u001b[A\n",
      "Training:   5%|██████▊                                                                                                                              | 154/3002 [10:29<3:03:02,  3.86s/it]\u001b[A\n",
      "Training:   5%|██████▊                                                                                                                              | 155/3002 [10:33<3:00:16,  3.80s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   5%|██████▉                                                                                                                              | 156/3002 [10:37<3:08:31,  3.97s/it]\u001b[A\n",
      "Training:   5%|██████▉                                                                                                                              | 157/3002 [10:41<3:09:48,  4.00s/it]\u001b[A\n",
      "Training:   5%|███████                                                                                                                              | 158/3002 [10:45<3:04:14,  3.89s/it]\u001b[A\n",
      "Training:   5%|███████                                                                                                                              | 159/3002 [10:49<3:12:38,  4.07s/it]\u001b[A\n",
      "Training:   5%|███████                                                                                                                              | 160/3002 [10:53<3:07:21,  3.96s/it]\u001b[A\n",
      "Training:   5%|███████▏                                                                                                                             | 161/3002 [10:57<3:09:31,  4.00s/it]\u001b[A\n",
      "Training:   5%|███████▏                                                                                                                             | 162/3002 [11:01<3:06:25,  3.94s/it]\u001b[A\n",
      "Training:   5%|███████▏                                                                                                                             | 163/3002 [11:05<3:10:28,  4.03s/it]\u001b[A\n",
      "Training:   5%|███████▎                                                                                                                             | 164/3002 [11:10<3:20:47,  4.25s/it]\u001b[A\n",
      "Training:   5%|███████▎                                                                                                                             | 165/3002 [11:14<3:14:53,  4.12s/it]\u001b[A\n",
      "Training:   6%|███████▎                                                                                                                             | 166/3002 [11:18<3:17:22,  4.18s/it]\u001b[A\n",
      "Training:   6%|███████▍                                                                                                                             | 167/3002 [11:23<3:23:30,  4.31s/it]\u001b[A\n",
      "Training:   6%|███████▍                                                                                                                             | 168/3002 [11:26<3:15:27,  4.14s/it]\u001b[A\n",
      "Training:   6%|███████▍                                                                                                                             | 169/3002 [11:30<3:13:11,  4.09s/it]\u001b[A\n",
      "Training:   6%|███████▌                                                                                                                             | 170/3002 [11:34<3:07:32,  3.97s/it]\u001b[A\n",
      "Training:   6%|███████▌                                                                                                                             | 171/3002 [11:39<3:18:13,  4.20s/it]\u001b[A\n",
      "Training:   6%|███████▌                                                                                                                             | 172/3002 [11:43<3:13:15,  4.10s/it]\u001b[A\n",
      "Training:   6%|███████▋                                                                                                                             | 173/3002 [11:47<3:16:01,  4.16s/it]\u001b[A\n",
      "Training:   6%|███████▋                                                                                                                             | 174/3002 [11:52<3:24:21,  4.34s/it]\u001b[A\n",
      "Training:   6%|███████▊                                                                                                                             | 175/3002 [11:56<3:22:12,  4.29s/it]\u001b[A\n",
      "Training:   6%|███████▊                                                                                                                             | 176/3002 [12:00<3:20:09,  4.25s/it]\u001b[A\n",
      "Training:   6%|███████▊                                                                                                                             | 177/3002 [12:04<3:22:26,  4.30s/it]\u001b[A\n",
      "Training:   6%|███████▉                                                                                                                             | 178/3002 [12:09<3:24:05,  4.34s/it]\u001b[A\n",
      "Training:   6%|███████▉                                                                                                                             | 179/3002 [12:13<3:20:33,  4.26s/it]\u001b[A\n",
      "Training:   6%|███████▉                                                                                                                             | 180/3002 [12:17<3:14:01,  4.13s/it]\u001b[A\n",
      "Training:   6%|████████                                                                                                                             | 181/3002 [12:21<3:14:17,  4.13s/it]\u001b[A\n",
      "Training:   6%|████████                                                                                                                             | 182/3002 [12:25<3:12:29,  4.10s/it]\u001b[A\n",
      "Training:   6%|████████                                                                                                                             | 183/3002 [12:29<3:16:35,  4.18s/it]\u001b[A\n",
      "Training:   6%|████████▏                                                                                                                            | 184/3002 [12:33<3:11:33,  4.08s/it]\u001b[A\n",
      "Training:   6%|████████▏                                                                                                                            | 185/3002 [12:38<3:20:37,  4.27s/it]\u001b[A\n",
      "Training:   6%|████████▏                                                                                                                            | 186/3002 [12:42<3:17:52,  4.22s/it]\u001b[A\n",
      "Training:   6%|████████▎                                                                                                                            | 187/3002 [12:46<3:22:17,  4.31s/it]\u001b[A\n",
      "Training:   6%|████████▎                                                                                                                            | 188/3002 [12:50<3:13:01,  4.12s/it]\u001b[A\n",
      "Training:   6%|████████▎                                                                                                                            | 189/3002 [12:54<3:09:32,  4.04s/it]\u001b[A\n",
      "Training:   6%|████████▍                                                                                                                            | 190/3002 [12:58<3:11:19,  4.08s/it]\u001b[A\n",
      "Training:   6%|████████▍                                                                                                                            | 191/3002 [13:02<3:11:08,  4.08s/it]\u001b[A\n",
      "Training:   6%|████████▌                                                                                                                            | 192/3002 [13:06<3:13:17,  4.13s/it]\u001b[A\n",
      "Training:   6%|████████▌                                                                                                                            | 193/3002 [13:10<3:10:11,  4.06s/it]\u001b[A\n",
      "Training:   6%|████████▌                                                                                                                            | 194/3002 [13:14<3:05:27,  3.96s/it]\u001b[A\n",
      "Training:   6%|████████▋                                                                                                                            | 195/3002 [13:18<3:08:00,  4.02s/it]\u001b[A\n",
      "Training:   7%|████████▋                                                                                                                            | 196/3002 [13:23<3:11:56,  4.10s/it]\u001b[A\n",
      "Training:   7%|████████▋                                                                                                                            | 197/3002 [13:28<3:27:36,  4.44s/it]\u001b[A\n",
      "Training:   7%|████████▊                                                                                                                            | 198/3002 [13:32<3:18:28,  4.25s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   7%|████████▊                                                                                                                            | 199/3002 [13:35<3:11:51,  4.11s/it]\u001b[A\n",
      "Training:   7%|████████▊                                                                                                                            | 200/3002 [13:39<3:08:10,  4.03s/it]\u001b[A\n",
      "Training:   7%|████████▉                                                                                                                            | 201/3002 [13:43<3:08:10,  4.03s/it]\u001b[A\n",
      "Training:   7%|████████▉                                                                                                                            | 202/3002 [13:48<3:21:46,  4.32s/it]\u001b[A\n",
      "Training:   7%|████████▉                                                                                                                            | 203/3002 [13:52<3:14:38,  4.17s/it]\u001b[A\n",
      "Training:   7%|█████████                                                                                                                            | 204/3002 [13:56<3:10:47,  4.09s/it]\u001b[A\n",
      "Training:   7%|█████████                                                                                                                            | 205/3002 [14:01<3:18:46,  4.26s/it]\u001b[A\n",
      "Training:   7%|█████████▏                                                                                                                           | 206/3002 [14:05<3:15:12,  4.19s/it]\u001b[A\n",
      "Training:   7%|█████████▏                                                                                                                           | 207/3002 [14:08<3:09:40,  4.07s/it]\u001b[A\n",
      "Training:   7%|█████████▏                                                                                                                           | 208/3002 [14:13<3:15:59,  4.21s/it]\u001b[A\n",
      "Training:   7%|█████████▎                                                                                                                           | 209/3002 [14:17<3:06:58,  4.02s/it]\u001b[A\n",
      "Training:   7%|█████████▎                                                                                                                           | 210/3002 [14:22<3:20:09,  4.30s/it]\u001b[A\n",
      "Training:   7%|█████████▎                                                                                                                           | 211/3002 [14:25<3:15:12,  4.20s/it]\u001b[A\n",
      "Training:   7%|█████████▍                                                                                                                           | 212/3002 [14:31<3:31:28,  4.55s/it]\u001b[A\n",
      "Training:   7%|█████████▍                                                                                                                           | 213/3002 [14:35<3:26:49,  4.45s/it]\u001b[A\n",
      "Training:   7%|█████████▍                                                                                                                           | 214/3002 [14:39<3:15:56,  4.22s/it]\u001b[A\n",
      "Training:   7%|█████████▌                                                                                                                           | 215/3002 [14:42<3:09:10,  4.07s/it]\u001b[A\n",
      "Training:   7%|█████████▌                                                                                                                           | 216/3002 [14:47<3:09:36,  4.08s/it]\u001b[A\n",
      "Training:   7%|█████████▌                                                                                                                           | 217/3002 [14:50<3:03:53,  3.96s/it]\u001b[A\n",
      "Training:   7%|█████████▋                                                                                                                           | 218/3002 [14:54<3:00:21,  3.89s/it]\u001b[A\n",
      "Training:   7%|█████████▋                                                                                                                           | 219/3002 [14:58<2:57:29,  3.83s/it]\u001b[A\n",
      "Training:   7%|█████████▋                                                                                                                           | 220/3002 [15:01<2:57:20,  3.82s/it]\u001b[A\n",
      "Training:   7%|█████████▊                                                                                                                           | 221/3002 [15:06<3:04:31,  3.98s/it]\u001b[A\n",
      "Training:   7%|█████████▊                                                                                                                           | 222/3002 [15:10<3:09:49,  4.10s/it]\u001b[A\n",
      "Training:   7%|█████████▉                                                                                                                           | 223/3002 [15:14<3:04:02,  3.97s/it]\u001b[A\n",
      "Training:   7%|█████████▉                                                                                                                           | 224/3002 [15:18<2:59:21,  3.87s/it]\u001b[A\n",
      "Training:   7%|█████████▉                                                                                                                           | 225/3002 [15:21<2:56:03,  3.80s/it]\u001b[A\n",
      "Training:   8%|██████████                                                                                                                           | 226/3002 [15:25<2:59:05,  3.87s/it]\u001b[A\n",
      "Training:   8%|██████████                                                                                                                           | 227/3002 [15:29<2:58:58,  3.87s/it]\u001b[A\n",
      "Training:   8%|██████████                                                                                                                           | 228/3002 [15:33<3:00:01,  3.89s/it]\u001b[A\n",
      "Training:   8%|██████████▏                                                                                                                          | 229/3002 [15:37<3:04:18,  3.99s/it]\u001b[A\n",
      "Training:   8%|██████████▏                                                                                                                          | 230/3002 [15:41<3:04:10,  3.99s/it]\u001b[A\n",
      "Training:   8%|██████████▏                                                                                                                          | 231/3002 [15:46<3:15:41,  4.24s/it]\u001b[A\n",
      "Training:   8%|██████████▎                                                                                                                          | 232/3002 [15:50<3:14:55,  4.22s/it]\u001b[A\n",
      "Training:   8%|██████████▎                                                                                                                          | 233/3002 [15:54<3:12:31,  4.17s/it]\u001b[A\n",
      "Training:   8%|██████████▎                                                                                                                          | 234/3002 [15:58<3:11:14,  4.15s/it]\u001b[A\n",
      "Training:   8%|██████████▍                                                                                                                          | 235/3002 [16:02<3:07:26,  4.06s/it]\u001b[A\n",
      "Training:   8%|██████████▍                                                                                                                          | 236/3002 [16:07<3:11:56,  4.16s/it]\u001b[A\n",
      "Training:   8%|██████████▌                                                                                                                          | 237/3002 [16:10<3:06:09,  4.04s/it]\u001b[A\n",
      "Training:   8%|██████████▌                                                                                                                          | 238/3002 [16:14<3:02:51,  3.97s/it]\u001b[A\n",
      "Training:   8%|██████████▌                                                                                                                          | 239/3002 [16:19<3:08:39,  4.10s/it]\u001b[A\n",
      "Training:   8%|██████████▋                                                                                                                          | 240/3002 [16:23<3:17:06,  4.28s/it]\u001b[A\n",
      "Training:   8%|██████████▋                                                                                                                          | 241/3002 [16:28<3:20:49,  4.36s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   8%|██████████▋                                                                                                                          | 242/3002 [16:31<3:10:10,  4.13s/it]\u001b[A\n",
      "Training:   8%|██████████▊                                                                                                                          | 243/3002 [16:37<3:26:39,  4.49s/it]\u001b[A\n",
      "Training:   8%|██████████▊                                                                                                                          | 244/3002 [16:41<3:18:44,  4.32s/it]\u001b[A\n",
      "Training:   8%|██████████▊                                                                                                                          | 245/3002 [16:44<3:10:15,  4.14s/it]\u001b[A\n",
      "Training:   8%|██████████▉                                                                                                                          | 246/3002 [16:49<3:10:06,  4.14s/it]\u001b[A\n",
      "Training:   8%|██████████▉                                                                                                                          | 247/3002 [16:52<3:07:07,  4.08s/it]\u001b[A\n",
      "Training:   8%|██████████▉                                                                                                                          | 248/3002 [16:56<3:03:25,  4.00s/it]\u001b[A\n",
      "Training:   8%|███████████                                                                                                                          | 249/3002 [17:01<3:19:34,  4.35s/it]\u001b[A\n",
      "Training:   8%|███████████                                                                                                                          | 250/3002 [17:05<3:09:55,  4.14s/it]\u001b[A\n",
      "Training:   8%|███████████                                                                                                                          | 251/3002 [17:09<3:08:54,  4.12s/it]\u001b[A\n",
      "Training:   8%|███████████▏                                                                                                                         | 252/3002 [17:13<3:10:49,  4.16s/it]\u001b[A\n",
      "Training:   8%|███████████▏                                                                                                                         | 253/3002 [17:18<3:18:42,  4.34s/it]\u001b[A\n",
      "Training:   8%|███████████▎                                                                                                                         | 254/3002 [17:22<3:08:48,  4.12s/it]\u001b[A\n",
      "Training:   8%|███████████▎                                                                                                                         | 255/3002 [17:26<3:08:14,  4.11s/it]\u001b[A\n",
      "Training:   9%|███████████▎                                                                                                                         | 256/3002 [17:30<3:13:23,  4.23s/it]\u001b[A\n",
      "Training:   9%|███████████▍                                                                                                                         | 257/3002 [17:34<3:07:03,  4.09s/it]\u001b[A\n",
      "Training:   9%|███████████▍                                                                                                                         | 258/3002 [17:38<3:01:03,  3.96s/it]\u001b[A\n",
      "Training:   9%|███████████▍                                                                                                                         | 259/3002 [17:42<3:03:12,  4.01s/it]\u001b[A\n",
      "Training:   9%|███████████▌                                                                                                                         | 260/3002 [17:50<3:08:07,  4.12s/it]\u001b[A\n",
      "  0%|                                                                                                                                                              | 0/1 [17:50<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/19/dgxwsbgd76728hx577833nx00000gn/T/ipykernel_1432/3250892457.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mbert_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid2label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model_path'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbert_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'head'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mf1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m print_log = f'''valid f1 score: {f1: .4f}, valid precision score: {precision: .4f},\n\u001b[1;32m      8\u001b[0m                 valid recall score: {recall: .4f}, valid accuracy score: {accuracy: .4f}'''\n",
      "\u001b[0;32m/var/folders/19/dgxwsbgd76728hx577833nx00000gn/T/ipykernel_1432/786933786.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(config, train_dataloader, val_dataloader, model)\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 模型参数梯度清零\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 反向传播\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 更新参数\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m             \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 叠加loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0mglobal_steps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ng/lib/python3.7/site-packages/torch/optim/lr_scheduler.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m                 \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m                 \u001b[0mwrapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0;31m# Note that the returned function here is no longer a bound method,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/code/git/fhaoguo/NLT/CLS/toutiao_news_cls/nezha_head_focalloss/extra_optim.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mgroup\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"counter\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ng/lib/python3.7/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ng/lib/python3.7/site-packages/transformers/optimization.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    365\u001b[0m                     \u001b[0mbias_correction1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"step\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m                     \u001b[0mbias_correction2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"step\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 367\u001b[0;31m                     \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep_size\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m                 \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcdiv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp_avg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdenom\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 首次运行代码\n",
    "bert_config = NeZhaConfig.from_pretrained(config['model_path'])\n",
    "bert_config.output_hidden_states = True\n",
    "bert_config.num_labels = len(id2label)\n",
    "model = build_model(config['model_path'], bert_config, config['head'])\n",
    "f1, precision, recall, accuracy = train(config, train_dataloader, val_dataloader, model)\n",
    "print_log = f'''valid f1 score: {f1: .4f}, valid precision score: {precision: .4f},\n",
    "                valid recall score: {recall: .4f}, valid accuracy score: {accuracy: .4f}'''\n",
    "print(print_log)\n",
    "\n",
    "# 迭代训练代码\n",
    "# bert_config = BertConfig.from_pretrained('../../../pt_tmp/cls/nezha_head_focal_loss')\n",
    "# bert_config.output_hidden_states = True\n",
    "# bert_config.num_labels = len(id2label)\n",
    "# model = build_model('../../../pt_tmp/cls/nezha_head_focal_loss', bert_config, config['head'])\n",
    "# f1, precision, recall, accuracy = train(config, train_dataloader, val_dataloader, model)\n",
    "# print_log = f'''valid f1 score: {f1: .4f}, valid precision score: {precision: .4f},\n",
    "#                 valid recall score: {recall: .4f}, valid accuracy score: {accuracy: .4f}'''\n",
    "# print(print_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8c2a95",
   "metadata": {},
   "source": [
    "## 预测并保存结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e47176d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(config, id2label, model, test_dataloader):\n",
    "    test_iterator = tqdm(test_dataloader, desc='Testing', total=len(test_dataloader))\n",
    "    model.eval()\n",
    "    test_preds = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in test_iterator:\n",
    "            batch = {item: value.to(config['device']) for item, value in batch.items()}\n",
    "\n",
    "            logits = model(**batch)[1]\n",
    "            test_preds.append(logits.argmax(dim=-1).detach().cpu())\n",
    "            \n",
    "    test_preds = torch.cat(test_preds, dim=0).numpy()\n",
    "    test_preds = [id2label[id_] for id_ in test_preds]\n",
    "        \n",
    "    test_df = pd.read_csv(config['test_file_path'], sep=',')\n",
    "    # test_df.insert(1, column=['label_pred'], value=test_preds)\n",
    "    test_df['label_pred'] = test_preds\n",
    "    # test_df.drop(columns=['sentence'], inplace=True)\n",
    "    test_df.to_csv('submission.csv', index=False, encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "60fde8bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|████████████████████████████████████████████| 625/625 [10:44<00:00,  1.03s/it]\n"
     ]
    }
   ],
   "source": [
    "predict(config, id2label, best_model, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "98bf771a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(config['test_file_path'], sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a4364da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(config['train_file_path'], sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9d665174",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>label_desc</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>108</td>\n",
       "      <td>news_edu</td>\n",
       "      <td>上课时学生手机响个不停，老师一怒之下把手机摔了，家长拿发票让老师赔，大家怎么看待这种事？</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>104</td>\n",
       "      <td>news_finance</td>\n",
       "      <td>商赢环球股份有限公司关于延期回复上海证券交易所对公司2017年年度报告的事后审核问询函的公告</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>106</td>\n",
       "      <td>news_house</td>\n",
       "      <td>通过中介公司买了二手房，首付都付了，现在卖家不想卖了。怎么处理？</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>112</td>\n",
       "      <td>news_travel</td>\n",
       "      <td>2018年去俄罗斯看世界杯得花多少钱？</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>109</td>\n",
       "      <td>news_tech</td>\n",
       "      <td>剃须刀的个性革新，雷明登天猫定制版新品首发</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>103</td>\n",
       "      <td>news_sports</td>\n",
       "      <td>再次证明了“无敌是多么寂寞”——逆天的中国乒乓球队！</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>109</td>\n",
       "      <td>news_tech</td>\n",
       "      <td>三农盾SACC-全球首个推出：互联网+区块链+农产品的电商平台</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>116</td>\n",
       "      <td>news_game</td>\n",
       "      <td>重做or新英雄？其实重做对暴雪来说同样重要</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>103</td>\n",
       "      <td>news_sports</td>\n",
       "      <td>如何在商业活动中不受人欺骗？</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>101</td>\n",
       "      <td>news_culture</td>\n",
       "      <td>87版红楼梦最温柔的四个丫鬟，娶谁都是一生的福气</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label    label_desc                                        sentence\n",
       "0   0    108      news_edu    上课时学生手机响个不停，老师一怒之下把手机摔了，家长拿发票让老师赔，大家怎么看待这种事？\n",
       "1   1    104  news_finance  商赢环球股份有限公司关于延期回复上海证券交易所对公司2017年年度报告的事后审核问询函的公告\n",
       "2   2    106    news_house                通过中介公司买了二手房，首付都付了，现在卖家不想卖了。怎么处理？\n",
       "3   3    112   news_travel                             2018年去俄罗斯看世界杯得花多少钱？\n",
       "4   4    109     news_tech                           剃须刀的个性革新，雷明登天猫定制版新品首发\n",
       "5   5    103   news_sports                      再次证明了“无敌是多么寂寞”——逆天的中国乒乓球队！\n",
       "6   6    109     news_tech                 三农盾SACC-全球首个推出：互联网+区块链+农产品的电商平台\n",
       "7   7    116     news_game                           重做or新英雄？其实重做对暴雪来说同样重要\n",
       "8   8    103   news_sports                                  如何在商业活动中不受人欺骗？\n",
       "9   9    101  news_culture                        87版红楼梦最温柔的四个丫鬟，娶谁都是一生的福气"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2e7757f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([108, 104, 106, 112, 109, 103, 116, 101, 107, 100, 102, 110, 115,\n",
       "       113, 114])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c321e381",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
