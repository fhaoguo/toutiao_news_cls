{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c40143c7",
   "metadata": {},
   "source": [
    "# 获取Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f9fbcd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bz2\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from icecream import ic\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "702dc931",
   "metadata": {},
   "outputs": [],
   "source": [
    "WORD_EMBEDDING_FILE = '../../../pt/sgns_weibo/sgns.weibo.word.bz2'\n",
    "token2embedding = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "815a9a48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load embedding file: ../../../pt/sgns_weibo/sgns.weibo.word.bz2 end!\n"
     ]
    }
   ],
   "source": [
    "with bz2.open(WORD_EMBEDDING_FILE) as f:\n",
    "    token_vectors = f.readlines()\n",
    "    vob_size, dim = token_vectors[0].split()\n",
    "\n",
    "print('load embedding file: {} end!'.format(WORD_EMBEDDING_FILE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98d8da50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(vocabulary: set):\n",
    "    for line in token_vectors[1:]:\n",
    "        tokens = line.split()\n",
    "        token = tokens[0].decode('utf-8')\n",
    "        if token in vocabulary:\n",
    "            token2embedding[token] = list(map(float, tokens[1:]))\n",
    "            assert len(token2embedding[token]) == int(dim)\n",
    "            \n",
    "    UNK, PAD, BOS, EOS = '<unk> <pad> <bos> <eos>'.split()\n",
    "    special_token_num = 4\n",
    "    token2id = {token: _id for _id, token in enumerate(token2embedding.keys(), special_token_num)}\n",
    "    \n",
    "    token2id[PAD] = 0\n",
    "    token2id[UNK] = 1\n",
    "    token2id[BOS] = 2\n",
    "    token2id[EOS] = 3\n",
    "    \n",
    "    id2vec = {token2id[token]: embedding for token, embedding in token2embedding.items()}\n",
    "    id2vec[0] = [0.] * int(dim)\n",
    "    id2vec[1] = [0.] * int(dim)\n",
    "    id2vec[2] = [random.uniform(-1, 1)] * int(dim)\n",
    "    id2vec[3] = [random.uniform(-1, 1)] * int(dim)\n",
    "    \n",
    "    embedding = [id2vec[_id] for _id in range(len(id2vec))]\n",
    "    \n",
    "    return torch.tensor(embedding, dtype=torch.float), token2id, len(vocabulary) + 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5fe21f",
   "metadata": {},
   "source": [
    "# 定义TextCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1791f975",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2bd24b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCNN(nn.Module):\n",
    "    def __init__(self, word_embedding, each_filter_num, filter_heights, drop_out, num_classes):\n",
    "        super(TextCNN, self).__init__()\n",
    "        self.embedding = nn.Embedding.from_pretrained(word_embedding, freeze=True)\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv2d(in_channels = 1, out_channels=each_filter_num, \n",
    "                     kernel_size=(h, word_embedding.shape[1]))\n",
    "            for h in filter_heights\n",
    "        ])\n",
    "        \n",
    "        self.dropout = nn.Dropout(drop_out)\n",
    "        self.fc = nn.Linear(each_filter_num * len(filter_heights), num_classes)\n",
    "        \n",
    "    def conv_and_pool(self, x, conv):\n",
    "        x = F.relu(conv(x)).squeeze(3)\n",
    "        x = F.max_pool1d(x, x.size(2)).squeeze(2)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def forward(self, input_ids=None):\n",
    "        word_embeddings = self.embedding(input_ids)\n",
    "        sentence_embedding = word_embeddings.unsqueeze(1)\n",
    "        \n",
    "        out = torch.cat([self.conv_and_pool(sentence_embedding, conv) for conv in self.convs], 1)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc(out)\n",
    "\n",
    "        outputs = (out, )\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "16d8b6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "some_text_sentence = '今天股市大跌，明天不知啥情况'\n",
    "words = list(jieba.cut(some_text_sentence))\n",
    "embedding, token2id, _ = get_embedding(set(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4591de02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[-0.0667, -0.2550, -0.1036,  0.0592, -0.1217, -0.0764,  0.0159,  0.0916,\n",
      "         -0.2430,  0.1574, -0.0650, -0.0265, -0.0346,  0.0223, -0.0917]],\n",
      "       grad_fn=<AddmmBackward0>),)\n"
     ]
    }
   ],
   "source": [
    "text_cnn_model = TextCNN(embedding, each_filter_num=128, filter_heights=[2, 3, 5], drop_out=0.3,\n",
    "                         num_classes=15)\n",
    "ids =[token2id[w] for w in words]\n",
    "\n",
    "out = text_cnn_model(torch.tensor([ids]))\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a79a6e4",
   "metadata": {},
   "source": [
    "# 定义data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "26fe7baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import jieba\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "from operator import add\n",
    "from functools import reduce\n",
    "from collections import Counter\n",
    "from torch.utils.data import DataLoader\n",
    "from icecream import ic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "91e98624",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_with_print(all_corpus):\n",
    "    add_with_print.i = 0\n",
    "\n",
    "    def _wrap(a, b):\n",
    "        add_with_print.i += 1\n",
    "        if add_with_print.i % 2000 == 0:\n",
    "            print('{}/{}'.format(add_with_print.i, len(all_corpus)), end=' ')\n",
    "        if add_with_print.i % 10000 == 0:\n",
    "            print()\n",
    "        return a + b\n",
    "\n",
    "    return _wrap\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7c07dadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_vocabulary(train_file_path, vocab_size):\n",
    "    CUT, SENTENCE = 'cut', 'sentence'\n",
    "\n",
    "    corpus = pd.read_csv(train_file_path)\n",
    "    corpus[CUT] = corpus[SENTENCE].apply(lambda s: ' '.join(list(jieba.cut(s))))\n",
    "    sentence_counters = map(Counter, map(lambda s: s.split(), corpus[CUT].values))\n",
    "    chose_words = reduce(add_with_print(corpus), sentence_counters).most_common(vocab_size)\n",
    "\n",
    "    return [w for w, _ in chose_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "da37dca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(sentence, vocab: dict):\n",
    "    UNK = 1\n",
    "    ids = [vocab.get(word, UNK) for word in jieba.cut(sentence)]\n",
    "\n",
    "    return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8b7cfe22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_data(train_file, vocab2ids):\n",
    "    val_ratio = 0.2\n",
    "    content = pd.read_csv(train_file)\n",
    "    num_val = int(len(content) * val_ratio)\n",
    "\n",
    "    LABEL, SENTENCE = 'label', 'sentence'\n",
    "\n",
    "    labels = content[LABEL].values\n",
    "    content['input_ids'] = content[SENTENCE].apply(lambda s: ' '.join([str(id_) for id_ in tokenizer(s, vocab2ids)]))\n",
    "    sentence_ids = np.array([[int(id_) for id_ in v.split()] for v in content['input_ids'].values])\n",
    "    \n",
    "    ids = np.random.choice(range(len(content)), size=len(content))\n",
    "    \n",
    "    train_ids = ids[num_val:]\n",
    "    val_ids = ids[:num_val]\n",
    "    \n",
    "    X_train, y_train = sentence_ids[train_ids], labels[train_ids]\n",
    "    X_val, y_val = sentence_ids[val_ids], labels[val_ids]\n",
    "\n",
    "    label2id = {label: i for i, label in enumerate(np.unique(y_train))}\n",
    "    id2label = {i: label for label, i in label2id.items()}\n",
    "    y_train = torch.tensor([label2id[y] for y in y_train], dtype=torch.long)\n",
    "    y_val = torch.tensor([label2id[y] for y in y_val], dtype=torch.long)\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, label2id, id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6079d78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataloader(X_train, y_train, X_val, y_val, batch_size):\n",
    "    train_dataloader = DataLoader([(x,y) for x,y in zip(X_train, y_train)],\n",
    "                                 batch_size=batch_size, num_workers=0, shuffle=True)\n",
    "    val_dataloader =DataLoader([(x,y) for x, y in zip(X_val, y_val)],\n",
    "                              batch_size=batch_size, num_workers=0, shuffle=True)\n",
    "    return train_dataloader, val_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "617d2c16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[list([5710, 5936, 6760, 1, 7, 38, 1921, 12, 1969, 242, 60, 7850, 14])\n",
      " list([9054, 8171, 3476, 1909, 404, 3182, 7, 79, 856, 28, 35, 2944, 15, 14, 4913, 19, 1, 144, 14])\n",
      " list([1, 49, 1, 40, 22, 1, 1, 1, 1]) ...\n",
      " list([1, 16, 1, 149, 1366, 8926, 14, 7137, 819, 1, 1, 4888, 160, 1, 24])\n",
      " list([7771, 2596, 17, 836, 1264, 754, 619, 507, 4699, 2591])\n",
      " list([1, 1, 1, 1, 2079, 944, 385, 2683, 2661, 7, 2072, 19, 54, 303, 14])] tensor([4, 6, 2,  ..., 3, 4, 7]) [list([1, 1, 2633, 24]) list([1, 1, 17, 49, 1, 40])\n",
      " list([1, 1, 188, 4426, 17, 166, 380, 1, 22, 2284, 7250, 4252, 7, 3382, 3663, 1246, 521, 1])\n",
      " ...\n",
      " list([1, 48, 4558, 1908, 36, 1309, 12, 1, 7, 4957, 20, 2685, 304, 498, 26, 5296, 4558, 24])\n",
      " list([3830, 12, 1942, 20, 1355, 1, 4198, 7, 1, 1, 2087, 1])\n",
      " list([1458, 25, 1, 1061, 29, 1479, 2477, 1, 1, 221, 2596, 71, 3152, 343, 6800, 7412])] tensor([ 9,  0,  9,  ..., 11, 10,  6]) {100: 0, 101: 1, 102: 2, 103: 3, 104: 4, 106: 5, 107: 6, 108: 7, 109: 8, 110: 9, 112: 10, 113: 11, 114: 12, 115: 13, 116: 14} {0: 100, 1: 101, 2: 102, 3: 103, 4: 104, 5: 106, 6: 107, 7: 108, 8: 109, 9: 110, 10: 112, 11: 113, 12: 114, 13: 115, 14: 116}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fenghaoguo/opt/anaconda3/envs/ng/lib/python3.7/site-packages/ipykernel_launcher.py:10: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "f = open('../../../data/toutiao_news_cls/vocabulary.txt', 'r')\n",
    "vocabulary = f.readlines()\n",
    "vocabulary = [v.strip() for v in vocabulary]\n",
    "embedding, token2id, vocab_size = get_embedding(set(vocabulary))\n",
    "X_train, y_train, X_val, y_val, label2id, id2label = get_train_data('../../../data/toutiao_news_cls/train.csv', vocab2ids=token2id)\n",
    "\n",
    "print(X_train, y_train, X_val, y_val, label2id, id2label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "869fba65",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "each element in list of batch should be of equal size",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/19/dgxwsbgd76728hx577833nx00000gn/T/ipykernel_2236/4006208975.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_dataloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ng/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ng/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ng/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/envs/ng/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'each element in list of batch should be of equal size'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0mtransposed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefault_collate_err_msg_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ng/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'each element in list of batch should be of equal size'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0mtransposed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefault_collate_err_msg_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ng/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0melem_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0melem_size\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0melem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'each element in list of batch should be of equal size'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m         \u001b[0mtransposed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: each element in list of batch should be of equal size"
     ]
    }
   ],
   "source": [
    "train_loader, val_loader = build_dataloader(X_train, y_train, X_val, y_val, batch_size=128)\n",
    "for i, (x, y) in enumerate(train_loader):\n",
    "    ic(x)\n",
    "    ic(y)\n",
    "    if i > 3: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459f1817",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
